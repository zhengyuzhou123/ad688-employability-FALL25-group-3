[
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Compute average salary by industry\navg_salary = (\n    df.groupby(industry_col)[salary_col]\n    .mean()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\n# Visual\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=avg_salary.values,\n    y=avg_salary.index,\n    orient=\"h\"\n)\nplt.title(\"Average Salary by Industry (Excluding Unclassified)\")\nplt.xlabel(\"Average Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\n\n# save\nplt.savefig(\"figures/average_salary_by_industry.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother bar chart was used to present the average salary across industries. Bar charts are effective for showing ranked values, making it easy to identify the highest-paying sectors and support data-driven career decisions.\nAverage pay skews toward specialized, capital-intensive niches. Polish & Other Sanitation Good Manufacturing tops the list (~$215k), followed by Rubber & Plastics Hoses/Belting and Agents for Wireless Telecommunications Services (~$185–$200k). A middle tier includes Irradiation Apparatus and Misc. Waste Management (~$175–$185k), while Web Search Portals, Plastics Bag/Pouch, and transport/retail (Rail Support, Electronics & Appliance Retailers, Wineries) cluster near the lower bound (~$150–$165k). Overall, excluding unclassified roles, high salaries concentrate in specialized manufacturing and regulated services.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "eda.html#exploratory-data-analysis-eda",
    "href": "eda.html#exploratory-data-analysis-eda",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Compute average salary by industry\navg_salary = (\n    df.groupby(industry_col)[salary_col]\n    .mean()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\n# Visual\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=avg_salary.values,\n    y=avg_salary.index,\n    orient=\"h\"\n)\nplt.title(\"Average Salary by Industry (Excluding Unclassified)\")\nplt.xlabel(\"Average Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\n\n# save\nplt.savefig(\"figures/average_salary_by_industry.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother bar chart was used to present the average salary across industries. Bar charts are effective for showing ranked values, making it easy to identify the highest-paying sectors and support data-driven career decisions.\nAverage pay skews toward specialized, capital-intensive niches. Polish & Other Sanitation Good Manufacturing tops the list (~$215k), followed by Rubber & Plastics Hoses/Belting and Agents for Wireless Telecommunications Services (~$185–$200k). A middle tier includes Irradiation Apparatus and Misc. Waste Management (~$175–$185k), while Web Search Portals, Plastics Bag/Pouch, and transport/retail (Rail Support, Electronics & Appliance Retailers, Wineries) cluster near the lower bound (~$150–$165k). Overall, excluding unclassified roles, high salaries concentrate in specialized manufacturing and regulated services.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "",
    "text": "Requirement already satisfied: gdown in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from gdown) (4.14.2)\nRequirement already satisfied: filelock in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from gdown) (3.20.0)\nRequirement already satisfied: requests[socks] in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from gdown) (2.32.5)\nRequirement already satisfied: tqdm in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve&gt;1.2 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from beautifulsoup4-&gt;gdown) (2.8)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from beautifulsoup4-&gt;gdown) (4.15.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (3.4.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (3.11)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (2025.11.12)\nRequirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from requests[socks]-&gt;gdown) (1.7.1)\nRequirement already satisfied: pandas in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (2.3.3)\nRequirement already satisfied: numpy&gt;=1.22.4 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from pandas) (2.2.6)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\nRequirement already satisfied: matplotlib in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (3.10.7)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler&gt;=0.10 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (4.61.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (1.4.9)\nRequirement already satisfied: numpy&gt;=1.23 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (2.2.6)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow&gt;=8 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (12.0.0)\nRequirement already satisfied: pyparsing&gt;=3 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (3.2.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six&gt;=1.5 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)\nRequirement already satisfied: missingno in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (0.5.2)\nRequirement already satisfied: numpy in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from missingno) (2.2.6)\nRequirement already satisfied: matplotlib in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from missingno) (3.10.7)\nRequirement already satisfied: scipy in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from missingno) (1.15.3)\nRequirement already satisfied: seaborn in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from missingno) (0.13.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib-&gt;missingno) (1.3.2)\nRequirement already satisfied: cycler&gt;=0.10 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib-&gt;missingno) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib-&gt;missingno) (4.61.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib-&gt;missingno) (1.4.9)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib-&gt;missingno) (25.0)\nRequirement already satisfied: pillow&gt;=8 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib-&gt;missingno) (12.0.0)\nRequirement already satisfied: pyparsing&gt;=3 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib-&gt;missingno) (3.2.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from matplotlib-&gt;missingno) (2.9.0.post0)\nRequirement already satisfied: six&gt;=1.5 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;missingno) (1.17.0)\nRequirement already satisfied: pandas&gt;=1.2 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from seaborn-&gt;missingno) (2.3.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;missingno) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from pandas&gt;=1.2-&gt;seaborn-&gt;missingno) (2025.2)\nRequirement already satisfied: pyarrow in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (22.0.0)\nRequirement already satisfied: scikit-learn in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (1.7.2)\nRequirement already satisfied: numpy&gt;=1.22.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\nRequirement already satisfied: scipy&gt;=1.8.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib&gt;=1.2.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    classification_report\n)\n\n# 1. Load the ORIGINAL Lightcast dataset\n#    (we do NOT use the cleaned subset here, because it only contains Remote jobs)\nraw_path = \"data/lightcast_job_postings.csv\"\ndf_raw = pd.read_csv(raw_path, low_memory=False)\n\n# 2. Clean the REMOTE_TYPE_NAME column\ndf_raw[\"REMOTE_TYPE_NAME\"] = (\n    df_raw[\"REMOTE_TYPE_NAME\"]\n    .astype(str)\n    .str.strip()\n)\n\nprint(\"Raw REMOTE_TYPE_NAME distribution (top 10):\")\nprint(df_raw[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False).head(10))\n\n# 3. Create a binary label: IS_REMOTE (1 = Remote-related, 0 = Non-Remote)\ndf_raw[\"IS_REMOTE\"] = df_raw[\"REMOTE_TYPE_NAME\"].str.contains(\n    \"remote\", case=False, na=False\n).astype(int)\n\nprint(\"\\nIS_REMOTE value counts:\")\nprint(df_raw[\"IS_REMOTE\"].value_counts())\n\n# If the data still has only one class, stop early to avoid model errors\nif df_raw[\"IS_REMOTE\"].nunique() &lt; 2:\n    raise ValueError(\n        \"The dataset currently contains only one class for IS_REMOTE. \"\n        \"Classification is not possible with a single class.\"\n    )\n\n# 4. Select features (structured, geography + industry)\nfeature_cols = [\"NAICS_2022_6_NAME\", \"STATE_NAME\"]\n\nfor col in feature_cols:\n    df_raw[col] = df_raw[col].astype(str).str.strip()\n\nX = df_raw[feature_cols]\ny = df_raw[\"IS_REMOTE\"]   # 0 = Non-Remote, 1 = Remote\n\n# 5. Train/Test split (stratify to keep class balance)\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.3,\n    random_state=42,\n    stratify=y\n)\n\n# 6. Preprocessing + Logistic Regression pipeline\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), feature_cols)\n    ]\n)\n\nclf = Pipeline(steps=[\n    (\"prep\", preprocess),\n    (\"logreg\", LogisticRegression(max_iter=300))\n])\n\n# 7. Fit the model\nclf.fit(X_train, y_train)\n\n# 8. Predictions and evaluation metrics\ny_pred = clf.predict(X_test)\n\nprint(f\"\\nAccuracy (Remote vs Non-Remote): {accuracy_score(y_test, y_pred):.3f}\\n\")\n\nprint(\"Classification report:\\n\")\nprint(classification_report(\n    y_test,\n    y_pred,\n    target_names=[\"Non-Remote\", \"Remote\"]\n))\n\n# 9. Confusion matrix plot (for the report)\ncm = confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Heatmap\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    cbar=True,\n    cbar_kws={\"shrink\": 0.8},  \n    xticklabels=[\"Non-Remote\", \"Remote\"],\n    yticklabels=[\"Non-Remote\", \"Remote\"],\n    ax=ax\n)\n\n# Set the title and coordinate axes\nax.set_title(\"Remote vs Non-Remote Job Classification\", fontsize=16, weight=\"bold\", pad=20)\nax.set_xlabel(\"Predicted label\", fontsize=12)\nax.set_ylabel(\"True label\", fontsize=12)\n\n# Set the colorbar label\ncbar = ax.collections[0].colorbar\ncbar.set_label(\"Count\", rotation=270, labelpad=15)\n\n# layout\nfig.tight_layout()\n\nplt.savefig(\"figures/remote_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nRaw REMOTE_TYPE_NAME distribution (top 10):\nREMOTE_TYPE_NAME\n[None]           56570\nRemote           12497\nHybrid Remote     2260\nNot Remote        1127\nnan                 44\nName: count, dtype: int64\n\nIS_REMOTE value counts:\nIS_REMOTE\n0    56614\n1    15884\nName: count, dtype: int64\n\nAccuracy (Remote vs Non-Remote): 0.793\n\nClassification report:\n\n              precision    recall  f1-score   support\n\n  Non-Remote       0.80      0.98      0.88     16985\n      Remote       0.64      0.13      0.21      4765\n\n    accuracy                           0.79     21750\n   macro avg       0.72      0.55      0.55     21750\nweighted avg       0.76      0.79      0.73     21750\n\n\n\n\n\n\n\n\n\n\nThis classifier can accurately identify most non-remote positions, but has difficulties in handling remote positions: many truly remote positions are wrongly classified as non-remote positions, indicating that merely relying on location and industry characteristics is not sufficient to capture the patterns suitable for remote work."
  },
  {
    "objectID": "ml_methods.html#salary-regression-impact-of-geography-and-remote-work",
    "href": "ml_methods.html#salary-regression-impact-of-geography-and-remote-work",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "2.1 Salary Regression: Impact of Geography and Remote Work",
    "text": "2.1 Salary Regression: Impact of Geography and Remote Work\nIn this section, we use a multiple linear regression model to predict job salary based on geographic location (state) and remote work type. This directly supports our topic of Geographic and Remote Work Analysis by quantifying how location and remote flexibility influence pay.\n\nimport os\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# 1. Load cleaned dataset\nDATA_PATH = \"data/cleaned_lightcast.csv\"\ndf = pd.read_csv(DATA_PATH, low_memory=False)\n\nprint(f\"Loaded rows: {len(df):,}, columns: {df.shape[1]}\")\n\n# 2. Choose columns for the regression model\n# Try to find a state column\nif \"STATE_NAME\" in df.columns:\n    state_col = \"STATE_NAME\"\nelif \"STATE\" in df.columns:\n    state_col = \"STATE\"\nelse:\n    raise ValueError(\"No state column found. Please check your dataset (STATE or STATE_NAME).\")\n\nremote_col = \"REMOTE_TYPE_NAME\"\nsalary_col = \"SALARY\"\n\n# 3. Basic cleaning: keep only useful columns and drop missing values\ndf_reg = df[[salary_col, state_col, remote_col]].copy()\n\n# Clean remote type text\ndf_reg[remote_col] = (\n    df_reg[remote_col]\n    .astype(str)\n    .str.strip()\n    .str.lower()\n)\n\n# Filter valid salary\ndf_reg = df_reg[df_reg[salary_col] &gt; 0].dropna()\n\nprint(f\"After filtering: {len(df_reg):,} rows\")\n\n# Optional: sample to speed up rendering\nif len(df_reg) &gt; 20000:\n    df_reg = df_reg.sample(n=20000, random_state=42)\n    print(f\"Sampled to 20,000 rows for faster modeling.\")\n\n# 4. Simplify remote type into three buckets\ndef map_remote(x: str) -&gt; str:\n    if \"remote\" in x:\n        return \"remote\"\n    if \"hybrid\" in x:\n        return \"hybrid\"\n    return \"onsite\"\n\ndf_reg[\"REMOTE_SIMPLE\"] = df_reg[remote_col].apply(map_remote)\n\n# 5. One-hot encode categorical variables\nmodel_df = df_reg[[salary_col, state_col, \"REMOTE_SIMPLE\"]].copy()\nmodel_df = pd.get_dummies(\n    model_df,\n    columns=[state_col, \"REMOTE_SIMPLE\"],\n    drop_first=True\n)\n\nX = model_df.drop(salary_col, axis=1)\ny = model_df[salary_col]\n\nprint(f\"Final feature columns: {X.shape[1]}\")\n\n# 6. Train/test split (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 7. Fit multiple linear regression\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\ny_pred = reg.predict(X_test)\n\n# 8. Evaluation metrics: RMSE & R²\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"RMSE: {rmse:,.2f}\")\nprint(f\"R²:   {r2:,.3f}\")\n\nLoaded rows: 69,198, columns: 118\nAfter filtering: 29,236 rows\nSampled to 20,000 rows for faster modeling.\nFinal feature columns: 51\nRMSE: 44,719.60\nR²:   0.010\n\n\n\n# 9. Coefficients as feature importance\ncoef_df = (\n    pd.DataFrame({\n        \"feature\": X.columns,\n        \"coefficient\": reg.coef_\n    })\n    .sort_values(\"coefficient\", ascending=False)\n)\n\nprint(coef_df.head(10))\n\n                     feature   coefficient\n28     STATE_NAME_New Jersey  12969.254218\n3      STATE_NAME_California  11557.354780\n44       STATE_NAME_Virginia  10826.522370\n5     STATE_NAME_Connecticut  10448.354346\n45     STATE_NAME_Washington   9536.253167\n43        STATE_NAME_Vermont   8935.467412\n2        STATE_NAME_Arkansas   7968.991975\n19  STATE_NAME_Massachusetts   7815.791150\n34       STATE_NAME_Oklahoma   7273.144350\n11       STATE_NAME_Illinois   7132.022318\n\n\n\n# 10. Plot feature importance (top 20 coefficients) with Matplotlib\n\ntop_n = 20\n\n#top20\ntop_coef = pd.concat([\n    coef_df.head(top_n),\n    coef_df.tail(top_n)\n])\n\nplt.figure(figsize=(8, 6))\nplt.barh(top_coef[\"feature\"], top_coef[\"coefficient\"])\nplt.axvline(0, color=\"black\", linewidth=0.8)\nplt.title(\"Top Positive and Negative Coefficients (Salary Regression)\")\nplt.xlabel(\"Coefficient\")\nplt.ylabel(\"Feature\")\nplt.tight_layout()\n\n# save png\nplt.savefig(\"figures/feature_importance.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# 11. Actual vs Predicted salary scatter plot (Matplotlib)\n\nscatter_df = pd.DataFrame({\n    \"Actual Salary\": y_test,\n    \"Predicted Salary\": y_pred\n})\n\nplt.figure(figsize=(6, 6))\nplt.scatter(\n    scatter_df[\"Actual Salary\"],\n    scatter_df[\"Predicted Salary\"],\n    alpha=0.4\n)\n\n# drave\nmin_val = min(scatter_df[\"Actual Salary\"].min(), scatter_df[\"Predicted Salary\"].min())\nmax_val = max(scatter_df[\"Actual Salary\"].max(), scatter_df[\"Predicted Salary\"].max())\nplt.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", linewidth=1)\n\nplt.title(\"Actual vs Predicted Salary\")\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.tight_layout()\n\n# save PNG\nplt.savefig(\"figures/actual_vs_predicted.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "data_analysis.html#data-cleaning-preprocessing",
    "href": "data_analysis.html#data-cleaning-preprocessing",
    "title": "Data Analysis",
    "section": "1 Data Cleaning & Preprocessing",
    "text": "1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\n\n1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\nimport seaborn as sns\n\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\n\n\n1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = df.columns.str.strip().str.upper()\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"SALARY\" in df.columns:\n        df[\"SALARY\"] = pd.to_numeric(\n            df[\"SALARY\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\n\n\n1.3 Visualize & handle missing values\n\n# --- Missing values correlation heatmap (like your classmate) ---\n\ncols_with_na = [c for c in df.columns if df[c].isna().any()]\nsub = df[cols_with_na]\n\nif len(cols_with_na) &gt;= 2:\n    \n    if len(cols_with_na) &gt; 25:\n        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index\n        sub = sub[top_na_cols]\n\n    \n    na_corr = sub.isna().astype(int).corr()\n\n    \n    mask = np.triu(np.ones_like(na_corr, dtype=bool))\n\n    plt.figure(figsize=(11, 8))\n    sns.heatmap(\n        na_corr, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1,\n        annot=True, fmt=\".1f\", linewidths=.5,\n        cbar_kws={\"shrink\": .8}\n    )\n    plt.title(\"Missing Values Correlation Heatmap\", fontsize=14)\n    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n\n    FIG_MISS = \"figures/missing_values_corr_heatmap.png\"\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    \nelse:\n    log(\"No columns with missing values; skipping heatmap.\")\n\n\n\n\n\n\n\n\n\n\n1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\n\n\n1.5 Optional salary sanity filter\n\nif \"SALARY\" in df.columns:\n    bad = (df[\"SALARY\"] &lt; 1) | (df[\"SALARY\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"SALARY\"] = np.nan\n        med2 = df[\"SALARY\"].median()\n        df[\"SALARY\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")"
  },
  {
    "objectID": "data_analysis.html#exploratory-data-analysis-eda",
    "href": "data_analysis.html#exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "2 Exploratory Data Analysis (EDA)",
    "text": "2 Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n2.1 Job Postings by Industry\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n2.2 Salary Distribution by Industry\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Compute average salary by industry\navg_salary = (\n    df.groupby(industry_col)[salary_col]\n    .mean()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\n# Visual\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=avg_salary.values,\n    y=avg_salary.index,\n    orient=\"h\"\n)\nplt.title(\"Average Salary by Industry (Excluding Unclassified)\")\nplt.xlabel(\"Average Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\n\n# save\nplt.savefig(\"figures/average_salary_by_industry.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother bar chart was used to present the average salary across industries. Bar charts are effective for showing ranked values, making it easy to identify the highest-paying sectors and support data-driven career decisions.\nAverage pay skews toward specialized, capital-intensive niches. Polish & Other Sanitation Good Manufacturing tops the list (~$215k), followed by Rubber & Plastics Hoses/Belting and Agents for Wireless Telecommunications Services (~$185–$200k). A middle tier includes Irradiation Apparatus and Misc. Waste Management (~$175–$185k), while Web Search Portals, Plastics Bag/Pouch, and transport/retail (Rail Support, Electronics & Appliance Retailers, Wineries) cluster near the lower bound (~$150–$165k). Overall, excluding unclassified roles, high salaries concentrate in specialized manufacturing and regulated services.\n\n\n2.3 Remote vs. On-Site Jobs\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "",
    "text": "Our team chose Geographic and Remote Work Analysis as our research topic. Since the outbreak of the pandemic, the rise of remote work, along with the continuous emergence of opportunities related to artificial intelligence, has changed the way job seekers evaluate career choices. At the same time, cities like Austin, Denver and Raleigh-Durham are leveraging flexible work policies and lower living costs to attract new talent."
  },
  {
    "objectID": "index.html#research-rationale",
    "href": "index.html#research-rationale",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "",
    "text": "Our team chose Geographic and Remote Work Analysis as our research topic. Since the outbreak of the pandemic, the rise of remote work, along with the continuous emergence of opportunities related to artificial intelligence, has changed the way job seekers evaluate career choices. At the same time, cities like Austin, Denver and Raleigh-Durham are leveraging flexible work policies and lower living costs to attract new talent."
  },
  {
    "objectID": "index.html#why-is-this-topic-important",
    "href": "index.html#why-is-this-topic-important",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "Why is this topic important?",
    "text": "Why is this topic important?\nIn today’s AI-dominated landscape, geographic location and skill sets have become pivotal factors in employment opportunities. As AI-driven technologies continue to enhance productivity, understanding the following aspects is critical: comparative job growth between AI and traditional sectors, hiring dynamics in major tech hubs such as Silicon Valley, Boston, and Austin, the urban-rural divide in AI versus non-AI career opportunities, and the transformation of remote work paradigms."
  },
  {
    "objectID": "index.html#trend-analysis",
    "href": "index.html#trend-analysis",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "Trend Analysis",
    "text": "Trend Analysis\nThis section examines geographic and remote work dynamics in the 2024–2025 U.S. labor market.\nAI-related jobs are highly concentrated in tech and policy hubs. Washington, D.C. leads with 4.4% of postings requiring AI skills, followed by Washington State and Delaware. At the city level, New York, Seattle, and San Jose dominate in absolute AI job postings, while D.C. has the highest proportion. Nationally, postings mentioning AI skills rose 20% year-over-year in 2024, with generative AI skills nearly quadrupling (Lightcast and Stanford HAI (2025), 1–17). In contrast, non-AI job growth is broad-based: 33 states reported employment gains in 2024, led by Rochester, MN (+6.5%) and Stockton–Lodi, CA (+5.3%), driven by healthcare and logistics (U.S. Bureau of Labor Statistics (2023), 1–2).\nRemote work has stabilized at a “new normal.” Although job postings labeled remote declined from &gt;10% in 2022 to ~7.5% in May 2025 (Indeed Hiring Lab (2024)), telework rates remain far above pre-pandemic levels, with ~20% of workers telecommuting in 2024 (U.S. Bureau of Labor Statistics (2024), 12). By industry, remote adoption is highest in IT, finance, and management (Makridis (2024), 16; Bick, Blandin, and Mertens (2024), 7)), while healthcare, retail, and manufacturing remain predominantly on-site. Employers are shifting toward hybrid models, often requiring more in-office presence, though remote-capable roles continue to play a significant role (Barrero, Bloom, and Davis (2023), 20–21).\nTraditional tech hubs like Silicon Valley and New York City remain dominant in high-skill hiring, with over 65% of AI engineers located in these regions as of 2024. Despite rising costs and the expansion of remote work, their strong innovation ecosystems and dense talent networks preserve their competitive edge. At the same time, geographic diversification is accelerating: Miami and San Diego are emerging as fast-growing hubs, driven by lifestyle appeal, lower living costs, and, in Miami’s case, favorable tax policies. Miami saw a 12% increase in AI roles, while San Diego experienced a 7% rise in Big Tech hiring and raised $5.7 billion in venture capital. Conversely, former growth leaders like Austin and Houston are losing momentum, with startup employment declining by 6% and 10.9%, respectively, due to infrastructure gaps, cultural mismatch, and a shift back toward hybrid work models (SignalFire (2025)).\nGenerative AI introduces a reversal in the geography of job market exposure compared to earlier automation technologies. Whereas prior waves of automation—such as robotics and enterprise software—primarily disrupted rural and small-town economies by replacing routine manual or physical labor, generative AI disproportionately affects urban, high-skill, white-collar labor markets. These urban centers, including San Jose, San Francisco, New York, and Washington D.C., are home to occupations rich in cognitive, nonroutine tasks such as coding, writing, and data analysis—tasks highly susceptible to AI augmentation or displacement(Muro, Methkupally, and Kinder (2025)).\nAs a result, AI exposure rates in urban counties often exceed 40%, while rural counties average closer to 30%, reflecting both differing occupational structures and varying access to information-oriented industries. While rural areas may be more insulated from AI disruption, they are also less likely to benefit from AI-driven productivity gains. This spatial shift in exposure demands new policy attention focused on urban workforce reskilling, while also ensuring rural communities are not left behind in the emerging AI economy(Muro, Methkupally, and Kinder (2025))."
  },
  {
    "objectID": "index.html#what-do-you-expect-to-find-in-your-analysis",
    "href": "index.html#what-do-you-expect-to-find-in-your-analysis",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "What do you expect to find in your analysis?",
    "text": "What do you expect to find in your analysis?\nOur research encompasses various aspects of geographic distribution and remote work trends. Specifically, we aim to investigate: Job growth patterns across different cities and states, including a comparative analysis of which metropolitan areas demonstrate the highest expansion rates for both AI-related and non-AI career opportunities. The evolution of remote work arrangements, examining whether remote positions are experiencing growth or decline and how this shift affects workforce distribution. Comprehensive analysis of tech hub dynamics, identifying whether traditional technology centers like Silicon Valley, Austin, and Boston continue to dominate the hiring landscape, or if emerging locations are gaining competitive advantages. The urban-rural divide in employment opportunities, investigating how job market conditions differ between metropolitan and rural areas for both AI and traditional professional roles, and what this means for workforce mobility and regional economic development."
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Analysis",
    "section": "",
    "text": "1 Create a team-based skill dataframe\n\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Bingrui Qiao\", \"Zhengyu Zhou\", \"Junhao Wang\"],\n    \"Python\": [2, 2, 3],\n    \"SQL\": [2, 2, 3],\n    \"Machine Learning\": [1, 1, 1],\n    \"Cloud Computing\": [2, 2, 3],\n    \"Docker\": [1, 1, 1],\n    \"AWS\": [2, 3, 2]\n}\n\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\nDocker\nAWS\n\n\nName\n\n\n\n\n\n\n\n\n\n\nBingrui Qiao\n2\n2\n1\n2\n1\n2\n\n\nZhengyu Zhou\n2\n2\n1\n2\n1\n3\n\n\nJunhao Wang\n3\n3\n1\n3\n1\n2\n\n\n\n\n\n\n\n\n\n2 Visualizing Skill Gaps\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#visual\nsns.set_theme(style=\"whitegrid\", font_scale=1.1, rc={\n    \"axes.facecolor\": \"white\",\n    \"figure.facecolor\": \"white\",\n    \"axes.edgecolor\": \"gray\",\n    \"grid.color\": \"lightgray\"\n})\n\n# Color choice\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# visual\nplt.figure(figsize=(9, 6))\nax = sns.heatmap(\n    df_skills,\n    annot=True,\n    fmt=\".0f\",\n    cmap=cmap,\n    linewidths=0.7,\n    cbar_kws={'label': 'Skill Level'},\n    square=True\n)\n\n# Title\nplt.title(\"Team Skill Levels Heatmap\", fontsize=15, weight=\"bold\", pad=20)\nplt.xlabel(\"Skill Domain\", fontsize=12)\nplt.ylabel(\"Team Member\", fontsize=12)\n\n# layout\nplt.xticks(rotation=30, ha=\"right\")\nplt.yticks(rotation=0)\nplt.tight_layout()\n\n#save\nplt.savefig(\"figures/Skill_Gap_Analysis.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3 Compare skill\n\n# Load data\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\")\n\n# Count keyword occurrences\ntop_skills = df_skills.columns.tolist()\njob_text = df[\"BODY\"].fillna(\"\")\nskill_counts = {s: job_text.str.contains(s, case=False).sum() for s in top_skills}\n\n# Append demand row\ndf_skills.loc[\"Market Demand\"] = [skill_counts[s] for s in top_skills]\ndf_skills\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\nDocker\nAWS\n\n\nName\n\n\n\n\n\n\n\n\n\n\nBingrui Qiao\n2\n2\n1\n2\n1\n2\n\n\nZhengyu Zhou\n2\n2\n1\n2\n1\n3\n\n\nJunhao Wang\n3\n3\n1\n3\n1\n2\n\n\nMarket Demand\n11782\n23202\n3972\n1302\n688\n14243\n\n\n\n\n\n\n\n\n\n4 Visual heatmap\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Same style\nsns.set_theme(style=\"whitegrid\", font_scale=1.1, rc={\n    \"axes.facecolor\": \"white\",\n    \"figure.facecolor\": \"white\",\n    \"axes.edgecolor\": \"gray\",\n    \"grid.color\": \"lightgray\"\n})\n\n\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# Team Skill Levels\nplt.figure(figsize=(8, 2.5))\nplt.imshow(df_skills.iloc[:-1], aspect=\"auto\", cmap=cmap)\nplt.colorbar(label=\"Skill Level (1–5)\")\nplt.yticks(range(len(df_skills.index)-1), df_skills.index[:-1], fontsize=10)\nplt.xticks(range(len(df_skills.columns)), df_skills.columns, rotation=30, ha=\"right\", fontsize=9)\nplt.title(\"Team Skill Levels\", fontsize=13, weight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"figures/Team_Skill_Level.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# Market Demand Heatmap\nplt.figure(figsize=(8, 2))\nplt.imshow([df_skills.loc[\"Market Demand\"]], aspect=\"auto\", cmap=cmap)\nplt.colorbar(label=\"Market Demand Count\")\nplt.yticks([0], [\"Market Demand\"], fontsize=10)\nplt.xticks(range(len(df_skills.columns)), df_skills.columns, rotation=30, ha=\"right\", fontsize=9)\nplt.title(\"Market Demand\", fontsize=13, weight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"figures/Market_Demand.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#data-cleaning-preprocessing",
    "href": "data_cleaning.html#data-cleaning-preprocessing",
    "title": "Data Cleaning",
    "section": "1 Data Cleaning & Preprocessing",
    "text": "1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\n\n1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\nimport seaborn as sns\n\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\n\n\n1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = df.columns.str.strip().str.upper()\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"SALARY\" in df.columns:\n        df[\"SALARY\"] = pd.to_numeric(\n            df[\"SALARY\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\n\n\n1.3 Visualize & handle missing values\n\n# --- Missing values correlation heatmap (like your classmate) ---\n\ncols_with_na = [c for c in df.columns if df[c].isna().any()]\nsub = df[cols_with_na]\n\nif len(cols_with_na) &gt;= 2:\n    \n    if len(cols_with_na) &gt; 25:\n        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index\n        sub = sub[top_na_cols]\n\n    \n    na_corr = sub.isna().astype(int).corr()\n\n    \n    mask = np.triu(np.ones_like(na_corr, dtype=bool))\n\n    plt.figure(figsize=(11, 8))\n    sns.heatmap(\n        na_corr, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1,\n        annot=True, fmt=\".1f\", linewidths=.5,\n        cbar_kws={\"shrink\": .8}\n    )\n    plt.title(\"Missing Values Correlation Heatmap\", fontsize=14)\n    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n\n    FIG_MISS = \"figures/missing_values_corr_heatmap.png\"\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    \nelse:\n    log(\"No columns with missing values; skipping heatmap.\")\n\n\n\n\n\n\n\n\n\n\n1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\n\n\n1.5 Optional salary sanity filter\n\nif \"SALARY\" in df.columns:\n    bad = (df[\"SALARY\"] &lt; 1) | (df[\"SALARY\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"SALARY\"] = np.nan\n        med2 = df[\"SALARY\"].median()\n        df[\"SALARY\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")"
  }
]