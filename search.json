[
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Compute average salary by industry\navg_salary = (\n    df.groupby(industry_col)[salary_col]\n    .mean()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\n# Visual\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=avg_salary.values,\n    y=avg_salary.index,\n    orient=\"h\"\n)\nplt.title(\"Average Salary by Industry (Excluding Unclassified)\")\nplt.xlabel(\"Average Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\n\n# save\nplt.savefig(\"figures/average_salary_by_industry.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother bar chart was used to present the average salary across industries. Bar charts are effective for showing ranked values, making it easy to identify the highest-paying sectors and support data-driven career decisions.\nAverage pay skews toward specialized, capital-intensive niches. Polish & Other Sanitation Good Manufacturing tops the list (~$215k), followed by Rubber & Plastics Hoses/Belting and Agents for Wireless Telecommunications Services (~$185–$200k). A middle tier includes Irradiation Apparatus and Misc. Waste Management (~$175–$185k), while Web Search Portals, Plastics Bag/Pouch, and transport/retail (Rail Support, Electronics & Appliance Retailers, Wineries) cluster near the lower bound (~$150–$165k). Overall, excluding unclassified roles, high salaries concentrate in specialized manufacturing and regulated services.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "eda.html#exploratory-data-analysis-eda",
    "href": "eda.html#exploratory-data-analysis-eda",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Compute average salary by industry\navg_salary = (\n    df.groupby(industry_col)[salary_col]\n    .mean()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\n# Visual\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=avg_salary.values,\n    y=avg_salary.index,\n    orient=\"h\"\n)\nplt.title(\"Average Salary by Industry (Excluding Unclassified)\")\nplt.xlabel(\"Average Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\n\n# save\nplt.savefig(\"figures/average_salary_by_industry.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother bar chart was used to present the average salary across industries. Bar charts are effective for showing ranked values, making it easy to identify the highest-paying sectors and support data-driven career decisions.\nAverage pay skews toward specialized, capital-intensive niches. Polish & Other Sanitation Good Manufacturing tops the list (~$215k), followed by Rubber & Plastics Hoses/Belting and Agents for Wireless Telecommunications Services (~$185–$200k). A middle tier includes Irradiation Apparatus and Misc. Waste Management (~$175–$185k), while Web Search Portals, Plastics Bag/Pouch, and transport/retail (Rail Support, Electronics & Appliance Retailers, Wineries) cluster near the lower bound (~$150–$165k). Overall, excluding unclassified roles, high salaries concentrate in specialized manufacturing and regulated services.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "nlp_methods.html",
    "href": "nlp_methods.html",
    "title": "1.import and setup",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Download nltk assets\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\n# Load cleaned dataset\nlightcast_data = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\nlightcast_data[\"BODY\"] = lightcast_data[\"BODY\"].fillna(\"\")\n\n[nltk_data] Downloading package stopwords to /home/runner/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/runner/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /home/runner/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n\n2.Basic Text Cleaning\n\n# Lowercase\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY\"].str.lower()\n\n# Remove non-letters and extra spaces\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY_lower\"].apply(\n    lambda x: re.sub(r\"[^a-z\\s]\", \" \", x)\n)\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY_lower\"].apply(\n    lambda x: re.sub(r\"\\s+\", \" \", x).strip()\n)\n\n\n\n3.Remove filler words\n\nfiller_words = [\n    \"the\",\"is\",\"in\",\"and\",\"to\",\"of\",\"a\",\"for\",\"on\",\"with\",\"as\",\"by\",\"at\",\"an\",\n    \"be\",\"this\",\"that\",\"it\",\"from\",\"or\"\n]\n\npattern = r'\\b(?:' + \"|\".join(filler_words) + r')\\b'\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY_lower\"].str.replace(pattern, \" \", regex=True)\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY_lower\"].str.replace(r\"\\s+\", \" \", regex=True)\n\n# Word count\nlightcast_data[\"BODY_word_count\"] = lightcast_data[\"BODY_lower\"].apply(lambda x: len(x.split()))\n\n\n\n4.Lemmatization\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_lemmatize(text):\n    tokens = text.split()\n    tokens = [w for w in tokens if w not in stop_words]\n    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n    return \" \".join(tokens)\n\nlightcast_data[\"BODY_lemmatized\"] = lightcast_data[\"BODY_lower\"].apply(preprocess_lemmatize)\n\n\n\n5.TF-IDF Vectorization\n\ntfidf_vectorizer = TfidfVectorizer(max_features=10000)\ntfidf_matrix = tfidf_vectorizer.fit_transform(lightcast_data[\"BODY_lemmatized\"])\n\nfeature_names = tfidf_vectorizer.get_feature_names_out()\ntfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n\nall_terms = pd.DataFrame({\n    \"term\": feature_names,\n    \"importance\": tfidf_scores\n}).sort_values(\"importance\", ascending=False)\n\n\n\n6.Remove meaningless high-frequency job description words\n\ngeneric_terms = {\n    \"data\",\"experience\",\"business\",\"job\",\"work\",\"team\",\"solution\",\"management\",\"system\",\n    \"client\",\"process\",\"year\",\"project\",\"ability\",\"position\",\"time\",\"service\",\"information\",\n    \"description\",\"requirement\",\"employee\",\"technical\",\"benefit\",\"customer\",\"application\",\n    \"need\",\"required\",\"preferred\",\"must\",\"strong\",\"sap\",\"skill\",\"analyst\",\"support\",\"technology\",\n    \"opportunity\",\"analysis\",\"enterprise\", \"including\",\"role\",\"development\", \"status\",\"full\",\"program\",\"tool\",\"may\",\"product\",\"knowledge\",\"company\",\"report\",\"new\", \"working\",\"oracle\",\"functional\",\"related\",\"provide\",\"quality\",\"implementation\",\"id\",\"help\",\"day\",\"pay\",\"employer\",\"reporting\",\"based\",\"applicant\",\"detail\",\"hour\",\"employment\",\"design\", \"industry\",\"develop\",\"lead\",\"candidate\",\"ensure\",\"plan\",\"disability\", \"architecture\",\"range\",\n    \"degree\", \"level\",\"insight\",\"user\",\"practice\",\"career\",\"qualification\",\"erp\",\"paid\"\n}\n\nfiltered_terms = all_terms[~all_terms[\"term\"].isin(generic_terms)]\ntop_keywords = filtered_terms.head(20)\n\ntop_keywords\n\n\n\n\n\n\n\n\nterm\nimportance\n\n\n\n\n454\nanalytics\n1407.540441\n\n\n1640\ncloud\n1351.994428\n\n\n4730\ninsurance\n1335.000934\n\n\n4202\nhealth\n1225.704218\n\n\n5338\nlocation\n1153.550292\n\n\n8046\nsecurity\n1135.234112\n\n\n3163\nenvironment\n1014.948480\n\n\n8474\nsql\n1004.168204\n\n\n8529\nstate\n994.774118\n\n\n313\nai\n987.901993\n\n\n3607\nfinancial\n965.079915\n\n\n6644\nperformance\n939.683032\n\n\n1978\nconsultant\n938.917781\n\n\n1761\ncommunication\n929.598457\n\n\n124\nacross\n918.248186\n\n\n613\narchitect\n912.098399\n\n\n8601\nstrategy\n909.582766\n\n\n4742\nintegration\n901.683705\n\n\n5258\nlife\n888.971965\n\n\n6376\norganization\n888.029278\n\n\n\n\n\n\n\n\n\nCreate wordcloud\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Convert top keywords into a dictionary {term: importance}\nword_freq = dict(zip(top_keywords[\"term\"], top_keywords[\"importance\"]))\n\n# Create the word cloud\nwc = WordCloud(\n    width=1200,\n    height=600,\n    background_color=\"white\",\n    colormap=\"viridis\",   \n    prefer_horizontal=0.9\n).generate_from_frequencies(word_freq)\n\n# Plot it\nplt.figure(figsize=(14, 7))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Top NLP Keywords from Job Descriptions\", fontsize=18)\nplt.tight_layout()\n\n#save\nplt.savefig(\"figures/WordCloud.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    classification_report\n)\n\n# 1. Load the ORIGINAL Lightcast dataset\n#    (we do NOT use the cleaned subset here, because it only contains Remote jobs)\nraw_path = \"data/lightcast_job_postings.csv\"\ndf_raw = pd.read_csv(raw_path, low_memory=False)\n\n# 2. Clean the REMOTE_TYPE_NAME column\ndf_raw[\"REMOTE_TYPE_NAME\"] = (\n    df_raw[\"REMOTE_TYPE_NAME\"]\n    .astype(str)\n    .str.strip()\n)\n\nprint(\"Raw REMOTE_TYPE_NAME distribution (top 10):\")\nprint(df_raw[\"REMOTE_TYPE_NAME\"].value_counts(dropna=False).head(10))\n\n# 3. Create a binary label: IS_REMOTE (1 = Remote-related, 0 = Non-Remote)\ndf_raw[\"IS_REMOTE\"] = df_raw[\"REMOTE_TYPE_NAME\"].str.contains(\n    \"remote\", case=False, na=False\n).astype(int)\n\nprint(\"\\nIS_REMOTE value counts:\")\nprint(df_raw[\"IS_REMOTE\"].value_counts())\n\n# If the data still has only one class, stop early to avoid model errors\nif df_raw[\"IS_REMOTE\"].nunique() &lt; 2:\n    raise ValueError(\n        \"The dataset currently contains only one class for IS_REMOTE. \"\n        \"Classification is not possible with a single class.\"\n    )\n\n# 4. Select features (structured, geography + industry)\nfeature_cols = [\"NAICS_2022_6_NAME\", \"STATE_NAME\"]\n\nfor col in feature_cols:\n    df_raw[col] = df_raw[col].astype(str).str.strip()\n\nX = df_raw[feature_cols]\ny = df_raw[\"IS_REMOTE\"]   # 0 = Non-Remote, 1 = Remote\n\n# 5. Train/Test split (stratify to keep class balance)\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.3,\n    random_state=42,\n    stratify=y\n)\n\n# 6. Preprocessing + Logistic Regression pipeline\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), feature_cols)\n    ]\n)\n\nclf = Pipeline(steps=[\n    (\"prep\", preprocess),\n    (\"logreg\", LogisticRegression(max_iter=300))\n])\n\n# 7. Fit the model\nclf.fit(X_train, y_train)\n\n# 8. Predictions and evaluation metrics\ny_pred = clf.predict(X_test)\n\nprint(f\"\\nAccuracy (Remote vs Non-Remote): {accuracy_score(y_test, y_pred):.3f}\\n\")\n\nprint(\"Classification report:\\n\")\nprint(classification_report(\n    y_test,\n    y_pred,\n    target_names=[\"Non-Remote\", \"Remote\"]\n))\n\n# 9. Confusion matrix plot (for the report)\ncm = confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Heatmap\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    cbar=True,\n    cbar_kws={\"shrink\": 0.8},  \n    xticklabels=[\"Non-Remote\", \"Remote\"],\n    yticklabels=[\"Non-Remote\", \"Remote\"],\n    ax=ax\n)\n\n# Set the title and coordinate axes\nax.set_title(\"Remote vs Non-Remote Job Classification\", fontsize=16, weight=\"bold\", pad=20)\nax.set_xlabel(\"Predicted label\", fontsize=12)\nax.set_ylabel(\"True label\", fontsize=12)\n\n# Set the colorbar label\ncbar = ax.collections[0].colorbar\ncbar.set_label(\"Count\", rotation=270, labelpad=15)\n\n# layout\nfig.tight_layout()\n\nplt.savefig(\"figures/remote_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nRaw REMOTE_TYPE_NAME distribution (top 10):\nREMOTE_TYPE_NAME\n[None]           56570\nRemote           12497\nHybrid Remote     2260\nNot Remote        1127\nnan                 44\nName: count, dtype: int64\n\nIS_REMOTE value counts:\nIS_REMOTE\n0    56614\n1    15884\nName: count, dtype: int64\n\nAccuracy (Remote vs Non-Remote): 0.793\n\nClassification report:\n\n              precision    recall  f1-score   support\n\n  Non-Remote       0.80      0.98      0.88     16985\n      Remote       0.64      0.13      0.21      4765\n\n    accuracy                           0.79     21750\n   macro avg       0.72      0.55      0.55     21750\nweighted avg       0.76      0.79      0.73     21750\n\n\n\n\n\n\n\n\n\n\nThis classifier can accurately identify most non-remote positions, but has difficulties in handling remote positions: many truly remote positions are wrongly classified as non-remote positions, indicating that merely relying on location and industry characteristics is not sufficient to capture the patterns suitable for remote work."
  },
  {
    "objectID": "ml_methods.html#feature-importance-analysis",
    "href": "ml_methods.html#feature-importance-analysis",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "2.1 Feature Importance Analysis",
    "text": "2.1 Feature Importance Analysis\n\n\n\nFeature Importance\n\n\nFigure: Top Positive and Negative Coefficients — This plot highlights the features that most strongly influence predicted salary levels. Positive coefficients indicate factors associated with higher predicted salaries, while negative coefficients indicate factors associated with lower predicted salaries.\nThe regression model shows that geographic differences play the largest role. States like Washington, D.C., California, and New York have strong positive coefficients, while states such as West Virginia and South Dakota show large negative coefficients. Remote/hybrid indicators have smaller effects relative to geography.\n\n# 11. Actual vs Predicted salary scatter plot (Matplotlib)\n\nscatter_df = pd.DataFrame({\n    \"Actual Salary\": y_test,\n    \"Predicted Salary\": y_pred\n})\n\nplt.figure(figsize=(6, 6))\nplt.scatter(\n    scatter_df[\"Actual Salary\"],\n    scatter_df[\"Predicted Salary\"],\n    alpha=0.4\n)\n\n# drave\nmin_val = min(scatter_df[\"Actual Salary\"].min(), scatter_df[\"Predicted Salary\"].min())\nmax_val = max(scatter_df[\"Actual Salary\"].max(), scatter_df[\"Predicted Salary\"].max())\nplt.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", linewidth=1)\n\nplt.title(\"Actual vs Predicted Salary\")\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.tight_layout()\n\n# save PNG\nplt.savefig(\"figures/actual_vs_predicted.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "ml_methods.html#actual-vs-predicted-salary",
    "href": "ml_methods.html#actual-vs-predicted-salary",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "2.2 Actual vs Predicted Salary",
    "text": "2.2 Actual vs Predicted Salary\n\n\n\nActual vs Predicted\n\n\nFigure: Actual vs Predicted Salary — Scatter plot comparing true salary with model predictions. The dashed line represents perfect prediction (y = x).\nThe prediction pattern shows that the model captures general salary trends, but underestimates extremely high salaries and compresses the overall range. This behavior is consistent with a moderate R² score and suggests that additional job-related variables (e.g., experience level, seniority) would improve model accuracy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "",
    "text": "Our team chose Geographic and Remote Work Analysis as our research topic. Since the outbreak of the pandemic, the rise of remote work, along with the continuous emergence of opportunities related to artificial intelligence, has changed the way job seekers evaluate career choices. At the same time, cities like Austin, Denver and Raleigh-Durham are leveraging flexible work policies and lower living costs to attract new talent."
  },
  {
    "objectID": "index.html#research-rationale",
    "href": "index.html#research-rationale",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "",
    "text": "Our team chose Geographic and Remote Work Analysis as our research topic. Since the outbreak of the pandemic, the rise of remote work, along with the continuous emergence of opportunities related to artificial intelligence, has changed the way job seekers evaluate career choices. At the same time, cities like Austin, Denver and Raleigh-Durham are leveraging flexible work policies and lower living costs to attract new talent."
  },
  {
    "objectID": "index.html#why-is-this-topic-important",
    "href": "index.html#why-is-this-topic-important",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "Why is this topic important?",
    "text": "Why is this topic important?\nIn today’s AI-dominated landscape, geographic location and skill sets have become pivotal factors in employment opportunities. As AI-driven technologies continue to enhance productivity, understanding the following aspects is critical: comparative job growth between AI and traditional sectors, hiring dynamics in major tech hubs such as Silicon Valley, Boston, and Austin, the urban-rural divide in AI versus non-AI career opportunities, and the transformation of remote work paradigms."
  },
  {
    "objectID": "index.html#trend-analysis",
    "href": "index.html#trend-analysis",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "Trend Analysis",
    "text": "Trend Analysis\nThis section examines geographic and remote work dynamics in the 2024–2025 U.S. labor market.\nAI-related jobs are highly concentrated in tech and policy hubs. Washington, D.C. leads with 4.4% of postings requiring AI skills, followed by Washington State and Delaware. At the city level, New York, Seattle, and San Jose dominate in absolute AI job postings, while D.C. has the highest proportion. Nationally, postings mentioning AI skills rose 20% year-over-year in 2024, with generative AI skills nearly quadrupling (Lightcast and Stanford HAI (2025), 1–17). In contrast, non-AI job growth is broad-based: 33 states reported employment gains in 2024, led by Rochester, MN (+6.5%) and Stockton–Lodi, CA (+5.3%), driven by healthcare and logistics (U.S. Bureau of Labor Statistics (2023), 1–2).\nRemote work has stabilized at a “new normal.” Although job postings labeled remote declined from &gt;10% in 2022 to ~7.5% in May 2025 (Indeed Hiring Lab (2024)), telework rates remain far above pre-pandemic levels, with ~20% of workers telecommuting in 2024 (U.S. Bureau of Labor Statistics (2024), 12). By industry, remote adoption is highest in IT, finance, and management (Makridis (2024), 16; Bick, Blandin, and Mertens (2024), 7)), while healthcare, retail, and manufacturing remain predominantly on-site. Employers are shifting toward hybrid models, often requiring more in-office presence, though remote-capable roles continue to play a significant role (Barrero, Bloom, and Davis (2023), 20–21).\nTraditional tech hubs like Silicon Valley and New York City remain dominant in high-skill hiring, with over 65% of AI engineers located in these regions as of 2024. Despite rising costs and the expansion of remote work, their strong innovation ecosystems and dense talent networks preserve their competitive edge. At the same time, geographic diversification is accelerating: Miami and San Diego are emerging as fast-growing hubs, driven by lifestyle appeal, lower living costs, and, in Miami’s case, favorable tax policies. Miami saw a 12% increase in AI roles, while San Diego experienced a 7% rise in Big Tech hiring and raised $5.7 billion in venture capital. Conversely, former growth leaders like Austin and Houston are losing momentum, with startup employment declining by 6% and 10.9%, respectively, due to infrastructure gaps, cultural mismatch, and a shift back toward hybrid work models (SignalFire (2025)).\nGenerative AI introduces a reversal in the geography of job market exposure compared to earlier automation technologies. Whereas prior waves of automation—such as robotics and enterprise software—primarily disrupted rural and small-town economies by replacing routine manual or physical labor, generative AI disproportionately affects urban, high-skill, white-collar labor markets. These urban centers, including San Jose, San Francisco, New York, and Washington D.C., are home to occupations rich in cognitive, nonroutine tasks such as coding, writing, and data analysis—tasks highly susceptible to AI augmentation or displacement(Muro, Methkupally, and Kinder (2025)).\nAs a result, AI exposure rates in urban counties often exceed 40%, while rural counties average closer to 30%, reflecting both differing occupational structures and varying access to information-oriented industries. While rural areas may be more insulated from AI disruption, they are also less likely to benefit from AI-driven productivity gains. This spatial shift in exposure demands new policy attention focused on urban workforce reskilling, while also ensuring rural communities are not left behind in the emerging AI economy(Muro, Methkupally, and Kinder (2025))."
  },
  {
    "objectID": "index.html#what-do-you-expect-to-find-in-your-analysis",
    "href": "index.html#what-do-you-expect-to-find-in-your-analysis",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "What do you expect to find in your analysis?",
    "text": "What do you expect to find in your analysis?\nOur research encompasses various aspects of geographic distribution and remote work trends. Specifically, we aim to investigate: Job growth patterns across different cities and states, including a comparative analysis of which metropolitan areas demonstrate the highest expansion rates for both AI-related and non-AI career opportunities. The evolution of remote work arrangements, examining whether remote positions are experiencing growth or decline and how this shift affects workforce distribution. Comprehensive analysis of tech hub dynamics, identifying whether traditional technology centers like Silicon Valley, Austin, and Boston continue to dominate the hiring landscape, or if emerging locations are gaining competitive advantages. The urban-rural divide in employment opportunities, investigating how job market conditions differ between metropolitan and rural areas for both AI and traditional professional roles, and what this means for workforce mobility and regional economic development."
  },
  {
    "objectID": "data_analysis.html#data-cleaning-preprocessing",
    "href": "data_analysis.html#data-cleaning-preprocessing",
    "title": "Data Analysis",
    "section": "1 Data Cleaning & Preprocessing",
    "text": "1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\n\n1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\nimport seaborn as sns\n\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\n\n\n1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = df.columns.str.strip().str.upper()\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"SALARY\" in df.columns:\n        df[\"SALARY\"] = pd.to_numeric(\n            df[\"SALARY\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\n\n\n1.3 Visualize & handle missing values\n\n# --- Missing values correlation heatmap (like your classmate) ---\n\ncols_with_na = [c for c in df.columns if df[c].isna().any()]\nsub = df[cols_with_na]\n\nif len(cols_with_na) &gt;= 2:\n    \n    if len(cols_with_na) &gt; 25:\n        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index\n        sub = sub[top_na_cols]\n\n    \n    na_corr = sub.isna().astype(int).corr()\n\n    \n    mask = np.triu(np.ones_like(na_corr, dtype=bool))\n\n    plt.figure(figsize=(11, 8))\n    sns.heatmap(\n        na_corr, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1,\n        annot=True, fmt=\".1f\", linewidths=.5,\n        cbar_kws={\"shrink\": .8}\n    )\n    plt.title(\"Missing Values Correlation Heatmap\", fontsize=14)\n    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n\n    FIG_MISS = \"figures/missing_values_corr_heatmap.png\"\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    \nelse:\n    log(\"No columns with missing values; skipping heatmap.\")\n\n\n\n\n\n\n\n\n\n\n1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\n\n\n1.5 Optional salary sanity filter\n\nif \"SALARY\" in df.columns:\n    bad = (df[\"SALARY\"] &lt; 1) | (df[\"SALARY\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"SALARY\"] = np.nan\n        med2 = df[\"SALARY\"].median()\n        df[\"SALARY\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")"
  },
  {
    "objectID": "data_analysis.html#exploratory-data-analysis-eda",
    "href": "data_analysis.html#exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "2 Exploratory Data Analysis (EDA)",
    "text": "2 Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n2.1 Job Postings by Industry\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n2.2 Salary Distribution by Industry\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Select top 10 industries by posting count\ntop10 = df[industry_col].value_counts().head(10).index\ndf_top10 = df[df[industry_col].isin(top10)]\n\n# --- Boxplot Visualization ---\npalette = sns.color_palette(\"Spectral\", n_colors=10)\n\nplt.figure(figsize=(12, 8))\n\nsns.boxplot(\n    data=df_top10,\n    y=industry_col,\n    x=salary_col,\n    palette=palette,\n    showfliers=True,     # show outlier\n    linewidth=1.2\n)\n\n# Add title & labels\nplt.title(\"Salary Distribution by Industry (Top 10 Industries)\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"Salary (USD)\", fontsize=14)\nplt.ylabel(\"Industry\", fontsize=14)\n\n# Light grid background like example\nsns.set_style(\"whitegrid\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n\nplt.tight_layout()\n\n# Save figure\nplt.savefig(\"figures/salary_distribution_by_industry_top10.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n/tmp/ipykernel_3556/3831417689.py:25: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nUse a box chart to show the salary distribution of each major recruitment industry. Unlike bar charts, bar charts only show averages, while box charts can reveal the median, fluctuation range and abnormal values in each industry. This helps job seekers not only understand which industries have higher average salaries, but also the extent of salary fluctuations and the areas with the greatest potential for salary growth.\nIn various industries, some knowledge-intensive and service-intensive fields (such as unclassified industries and customized computer programming services) have relatively high salary levels and are more dispersed. The median salary in these areas is relatively high, and the upper tail is long, which indicates that there are a large number of highly paid positions. Management consulting, computer system design, and direct health and medical insurance industries also show a large range of remuneration, reflecting the wide range of remuneration within these professions. In contrast, industries such as colleges and universities, certified public accountants, commercial banks and software publishers have a tighter scope and fewer extreme outliers, indicating that the remuneration range of these industries is more standardized and there is less room for improvement at the top of the salary distribution.\n\n\n2.3 Remote vs. On-Site Jobs\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "final_report.html",
    "href": "final_report.html",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "",
    "text": "x"
  },
  {
    "objectID": "final_report.html#data-cleaning-preprocessing-1",
    "href": "final_report.html#data-cleaning-preprocessing-1",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "2.1 Data Cleaning & Preprocessing",
    "text": "2.1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\n\n2.1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\nimport seaborn as sns\n\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\n\n\n2.1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = df.columns.str.strip().str.upper()\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"SALARY\" in df.columns:\n        df[\"SALARY\"] = pd.to_numeric(\n            df[\"SALARY\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\n\n\n2.1.3 Visualize & handle missing values\n\n# --- Missing values correlation heatmap (like your classmate) ---\n\ncols_with_na = [c for c in df.columns if df[c].isna().any()]\nsub = df[cols_with_na]\n\nif len(cols_with_na) &gt;= 2:\n    \n    if len(cols_with_na) &gt; 25:\n        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index\n        sub = sub[top_na_cols]\n\n    \n    na_corr = sub.isna().astype(int).corr()\n\n    \n    mask = np.triu(np.ones_like(na_corr, dtype=bool))\n\n    plt.figure(figsize=(11, 8))\n    sns.heatmap(\n        na_corr, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1,\n        annot=True, fmt=\".1f\", linewidths=.5,\n        cbar_kws={\"shrink\": .8}\n    )\n    plt.title(\"Missing Values Correlation Heatmap\", fontsize=14)\n    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n\n    FIG_MISS = \"figures/missing_values_corr_heatmap.png\"\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    \nelse:\n    log(\"No columns with missing values; skipping heatmap.\")\n\n\n\n\n\n\n\n\n\n\n2.1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\n\n\n2.1.5 Optional salary sanity filter\n\nif \"SALARY\" in df.columns:\n    bad = (df[\"SALARY\"] &lt; 1) | (df[\"SALARY\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"SALARY\"] = np.nan\n        med2 = df[\"SALARY\"].median()\n        df[\"SALARY\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n2.1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")"
  },
  {
    "objectID": "final_report.html#exploratory-data-analysis-eda",
    "href": "final_report.html#exploratory-data-analysis-eda",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "2.2 Exploratory Data Analysis (EDA)",
    "text": "2.2 Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n2.2.1 Job Postings by Industry\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n2.2.2 Salary Distribution by Industry\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Select top 10 industries by posting count\ntop10 = df[industry_col].value_counts().head(10).index\ndf_top10 = df[df[industry_col].isin(top10)]\n\n# --- Boxplot Visualization ---\npalette = sns.color_palette(\"Spectral\", n_colors=10)\n\nplt.figure(figsize=(12, 8))\n\nsns.boxplot(\n    data=df_top10,\n    y=industry_col,\n    x=salary_col,\n    palette=palette,\n    showfliers=True,     # show outlier\n    linewidth=1.2\n)\n\n# Add title & labels\nplt.title(\"Salary Distribution by Industry (Top 10 Industries)\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"Salary (USD)\", fontsize=14)\nplt.ylabel(\"Industry\", fontsize=14)\n\n# Light grid background like example\nsns.set_style(\"whitegrid\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n\nplt.tight_layout()\n\n# Save figure\nplt.savefig(\"figures/salary_distribution_by_industry_top10.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n/tmp/ipykernel_3741/3831417689.py:25: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nUse a box chart to show the salary distribution of each major recruitment industry. Unlike bar charts, bar charts only show averages, while box charts can reveal the median, fluctuation range and abnormal values in each industry. This helps job seekers not only understand which industries have higher average salaries, but also the extent of salary fluctuations and the areas with the greatest potential for salary growth.\nIn various industries, some knowledge-intensive and service-intensive fields (such as unclassified industries and customized computer programming services) have relatively high salary levels and are more dispersed. The median salary in these areas is relatively high, and the upper tail is long, which indicates that there are a large number of highly paid positions. Management consulting, computer system design, and direct health and medical insurance industries also show a large range of remuneration, reflecting the wide range of remuneration within these professions. In contrast, industries such as colleges and universities, certified public accountants, commercial banks and software publishers have a tighter scope and fewer extreme outliers, indicating that the remuneration range of these industries is more standardized and there is less room for improvement at the top of the salary distribution.\n\n\n2.2.3 Remote vs. On-Site Jobs\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "final_report.html#feature-importance-analysis",
    "href": "final_report.html#feature-importance-analysis",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "10.1 Feature Importance Analysis",
    "text": "10.1 Feature Importance Analysis\n\n\n\nFeature Importance\n\n\nFigure: Top Positive and Negative Coefficients — This plot highlights the features that most strongly influence predicted salary levels. Positive coefficients indicate factors associated with higher predicted salaries, while negative coefficients indicate factors associated with lower predicted salaries.\nThe regression model shows that geographic differences play the largest role. States like Washington, D.C., California, and New York have strong positive coefficients, while states such as West Virginia and South Dakota show large negative coefficients. Remote/hybrid indicators have smaller effects relative to geography.\n\n# 11. Actual vs Predicted salary scatter plot (Matplotlib)\n\nscatter_df = pd.DataFrame({\n    \"Actual Salary\": y_test,\n    \"Predicted Salary\": y_pred\n})\n\nplt.figure(figsize=(6, 6))\nplt.scatter(\n    scatter_df[\"Actual Salary\"],\n    scatter_df[\"Predicted Salary\"],\n    alpha=0.4\n)\n\n# drave\nmin_val = min(scatter_df[\"Actual Salary\"].min(), scatter_df[\"Predicted Salary\"].min())\nmax_val = max(scatter_df[\"Actual Salary\"].max(), scatter_df[\"Predicted Salary\"].max())\nplt.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", linewidth=1)\n\nplt.title(\"Actual vs Predicted Salary\")\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.tight_layout()\n\n# save PNG\nplt.savefig(\"figures/actual_vs_predicted.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "final_report.html#actual-vs-predicted-salary",
    "href": "final_report.html#actual-vs-predicted-salary",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "10.2 Actual vs Predicted Salary",
    "text": "10.2 Actual vs Predicted Salary\n\n\n\nActual vs Predicted\n\n\nFigure: Actual vs Predicted Salary — Scatter plot comparing true salary with model predictions. The dashed line represents perfect prediction (y = x).\nThe prediction pattern shows that the model captures general salary trends, but underestimates extremely high salaries and compresses the overall range. This behavior is consistent with a moderate R² score and suggests that additional job-related variables (e.g., experience level, seniority) would improve model accuracy."
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Analysis",
    "section": "",
    "text": "1 Create a team-based skill dataframe\n\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Bingrui Qiao\", \"Zhengyu Zhou\", \"Junhao Wang\"],\n    \"Python\": [2, 2, 3],\n    \"SQL\": [2, 2, 3],\n    \"Machine Learning\": [1, 1, 1],\n    \"Cloud Computing\": [2, 2, 3],\n    \"Docker\": [1, 1, 1],\n    \"AWS\": [2, 3, 2]\n}\n\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\nDocker\nAWS\n\n\nName\n\n\n\n\n\n\n\n\n\n\nBingrui Qiao\n2\n2\n1\n2\n1\n2\n\n\nZhengyu Zhou\n2\n2\n1\n2\n1\n3\n\n\nJunhao Wang\n3\n3\n1\n3\n1\n2\n\n\n\n\n\n\n\n\n\n2 Visualizing Skill Gaps\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#visual\nsns.set_theme(style=\"whitegrid\", font_scale=1.1, rc={\n    \"axes.facecolor\": \"white\",\n    \"figure.facecolor\": \"white\",\n    \"axes.edgecolor\": \"gray\",\n    \"grid.color\": \"lightgray\"\n})\n\n# Color choice\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# visual\nplt.figure(figsize=(9, 6))\nax = sns.heatmap(\n    df_skills,\n    annot=True,\n    fmt=\".0f\",\n    cmap=cmap,\n    linewidths=0.7,\n    cbar_kws={'label': 'Skill Level'},\n    square=True\n)\n\n# Title\nplt.title(\"Team Skill Levels Heatmap\", fontsize=15, weight=\"bold\", pad=20)\nplt.xlabel(\"Skill Domain\", fontsize=12)\nplt.ylabel(\"Team Member\", fontsize=12)\n\n# layout\nplt.xticks(rotation=30, ha=\"right\")\nplt.yticks(rotation=0)\nplt.tight_layout()\n\n#save\nplt.savefig(\"figures/Skill_Gap_Analysis.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3 Compare skill\n\n# Load data\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\")\n\n# Count keyword occurrences\ntop_skills = df_skills.columns.tolist()\njob_text = df[\"BODY\"].fillna(\"\")\nskill_counts = {s: job_text.str.contains(s, case=False).sum() for s in top_skills}\n\n# Append demand row\ndf_skills.loc[\"Market Demand\"] = [skill_counts[s] for s in top_skills]\ndf_skills\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\nDocker\nAWS\n\n\nName\n\n\n\n\n\n\n\n\n\n\nBingrui Qiao\n2\n2\n1\n2\n1\n2\n\n\nZhengyu Zhou\n2\n2\n1\n2\n1\n3\n\n\nJunhao Wang\n3\n3\n1\n3\n1\n2\n\n\nMarket Demand\n11782\n23202\n3972\n1302\n688\n14243\n\n\n\n\n\n\n\n\n\n4 Visual heatmap\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Same style\nsns.set_theme(style=\"whitegrid\", font_scale=1.1, rc={\n    \"axes.facecolor\": \"white\",\n    \"figure.facecolor\": \"white\",\n    \"axes.edgecolor\": \"gray\",\n    \"grid.color\": \"lightgray\"\n})\n\n\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# Team Skill Levels\nplt.figure(figsize=(8, 2.5))\nplt.imshow(df_skills.iloc[:-1], aspect=\"auto\", cmap=cmap)\nplt.colorbar(label=\"Skill Level (1–5)\")\nplt.yticks(range(len(df_skills.index)-1), df_skills.index[:-1], fontsize=10)\nplt.xticks(range(len(df_skills.columns)), df_skills.columns, rotation=30, ha=\"right\", fontsize=9)\nplt.title(\"Team Skill Levels\", fontsize=13, weight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"figures/Team_Skill_Level.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# Market Demand Heatmap\nplt.figure(figsize=(8, 2))\nplt.imshow([df_skills.loc[\"Market Demand\"]], aspect=\"auto\", cmap=cmap)\nplt.colorbar(label=\"Market Demand Count\")\nplt.yticks([0], [\"Market Demand\"], fontsize=10)\nplt.xticks(range(len(df_skills.columns)), df_skills.columns, rotation=30, ha=\"right\", fontsize=9)\nplt.title(\"Market Demand\", fontsize=13, weight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"figures/Market_Demand.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#data-cleaning-preprocessing",
    "href": "data_cleaning.html#data-cleaning-preprocessing",
    "title": "Data Cleaning",
    "section": "1 Data Cleaning & Preprocessing",
    "text": "1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\n\n1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\nimport seaborn as sns\n\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\n\n\n1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = df.columns.str.strip().str.upper()\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"SALARY\" in df.columns:\n        df[\"SALARY\"] = pd.to_numeric(\n            df[\"SALARY\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\n\n\n1.3 Visualize & handle missing values\n\n# --- Missing values correlation heatmap (like your classmate) ---\n\ncols_with_na = [c for c in df.columns if df[c].isna().any()]\nsub = df[cols_with_na]\n\nif len(cols_with_na) &gt;= 2:\n    \n    if len(cols_with_na) &gt; 25:\n        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index\n        sub = sub[top_na_cols]\n\n    \n    na_corr = sub.isna().astype(int).corr()\n\n    \n    mask = np.triu(np.ones_like(na_corr, dtype=bool))\n\n    plt.figure(figsize=(11, 8))\n    sns.heatmap(\n        na_corr, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1,\n        annot=True, fmt=\".1f\", linewidths=.5,\n        cbar_kws={\"shrink\": .8}\n    )\n    plt.title(\"Missing Values Correlation Heatmap\", fontsize=14)\n    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n\n    FIG_MISS = \"figures/missing_values_corr_heatmap.png\"\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    \nelse:\n    log(\"No columns with missing values; skipping heatmap.\")\n\n\n\n\n\n\n\n\n\n\n1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\n\n\n1.5 Optional salary sanity filter\n\nif \"SALARY\" in df.columns:\n    bad = (df[\"SALARY\"] &lt; 1) | (df[\"SALARY\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"SALARY\"] = np.nan\n        med2 = df[\"SALARY\"].median()\n        df[\"SALARY\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")"
  }
]