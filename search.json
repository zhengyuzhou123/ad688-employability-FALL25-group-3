[
  {
    "objectID": "data_analysis.html#data-cleaning-preprocessing",
    "href": "data_analysis.html#data-cleaning-preprocessing",
    "title": "Data Analysis",
    "section": "1 Data Cleaning & Preprocessing",
    "text": "1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\nWROTE: logs/_ping.txt\n\n\n\n1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    log(f\"⚠️ Dataset missing at {DATA_PATH}. Attempting to download from Google Drive...\")\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n        log(\"✅ Dataset downloaded successfully.\")\n    except Exception as e:\n        raise FileNotFoundError(f\"❌ Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"❌ Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\nLoaded dataset → rows: 72,498, cols: 131\n\n\n\n\n1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = [c.strip() for c in df.columns]\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"Salary\" in df.columns:\n        df[\"Salary\"] = pd.to_numeric(\n            df[\"Salary\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\nDropped 13 columns; remaining columns: 118\n\n\n\n\n1.3 Visualize & handle missing values\n\nif HAS_MSNO:\n    plt.figure(figsize=(10,6))\n    msno.heatmap(df)\n    plt.title(\"Missing Values Heatmap\", fontsize=14)\n    plt.tight_layout()\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    log(f\"Saved missingness heatmap → {FIG_MISS}\")\nelse:\n    log(\"missingno not installed; skipping heatmap.\")\n\n# Log top-20 missing columns\n\nna_pct = df.isna().mean().sort_values(ascending=False) * 100\nlog(\"Top-20 columns by missing%:\")\nfor col, pct in na_pct.head(20).items():\n    log(f\"  - {col}: {pct:.1f}%\")\n\n# Drop columns with &gt;50% missing\n\nthresh = len(df) * 0.5\nbefore_cols = df.shape[1]\ndf.dropna(axis=1, thresh=thresh, inplace=True)\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} high-missing columns (&gt;50%). Remaining: {after_cols}\")\n\n# Impute numeric\n\nif \"Salary\" in df.columns:\n    med_salary = df[\"Salary\"].median()\n    df[\"Salary\"].fillna(med_salary, inplace=True)\n    log(f\"Filled Salary NAs with median: {med_salary:.2f}\")\n\n# Impute categorical\n\nfor cat in [\"Industry\",\"REMOTE_TYPE_NAME\",\"LOCATION\"]:\n    if cat in df.columns:\n        df[cat] = df[cat].fillna(\"Unknown\").astype(str).str.strip()\n        log(f\"Filled {cat} NAs with 'Unknown'\")\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nSaved missingness heatmap → figures/missing_values_heatmap.png\nTop-20 columns by missing%:\n  - ACTIVE_SOURCES_INFO: 89.2%\n  - MAX_YEARS_EXPERIENCE: 88.4%\n  - MAX_EDULEVELS: 77.5%\n  - MAX_EDULEVELS_NAME: 77.5%\n  - LIGHTCAST_SECTORS: 75.5%\n  - LIGHTCAST_SECTORS_NAME: 75.5%\n  - SALARY: 57.5%\n  - SALARY_FROM: 55.3%\n  - SALARY_TO: 55.3%\n  - ORIGINAL_PAY_PERIOD: 55.3%\n  - DURATION: 37.7%\n  - MIN_YEARS_EXPERIENCE: 31.9%\n  - MODELED_DURATION: 26.6%\n  - MODELED_EXPIRED: 21.3%\n  - EXPIRED: 10.8%\n  - MSA_INCOMING: 5.5%\n  - MSA_NAME_INCOMING: 5.5%\n  - MSA: 5.4%\n  - MSA_NAME_OUTGOING: 5.4%\n  - MSA_OUTGOING: 5.4%\nDropped 10 high-missing columns (&gt;50%). Remaining: 108\nFilled REMOTE_TYPE_NAME NAs with 'Unknown'\nFilled LOCATION NAs with 'Unknown'\n\n\n\n\n1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\nRemoved duplicates by ['TITLE', 'COMPANY', 'LOCATION', 'POSTED']: 3300 rows dropped; remaining: 69198\n\n\n\n\n1.5 Optional salary sanity filter\n\nif \"Salary\" in df.columns:\n    bad = (df[\"Salary\"] &lt; 1) | (df[\"Salary\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"Salary\"] = np.nan\n        med2 = df[\"Salary\"].median()\n        df[\"Salary\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")\n\nSaved cleaned dataset → data/cleaned_lightcast.csv\nRows: 69,198\nColumns: 108\nSample columns: ['LAST_UPDATED_DATE', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME']\n✅ Cleaning pipeline finished successfully."
  },
  {
    "objectID": "data_analysis.html#exploratory-data-analysis-eda",
    "href": "data_analysis.html#exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "2 5 Exploratory Data Analysis (EDA)",
    "text": "2 5 Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n2.1 5.1 Job Postings by Industry\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.2 5.2 Salary Distribution by Industry\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\ndf = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Compute average salary by industry\navg_salary = (\n    df.groupby(industry_col)[salary_col]\n    .mean()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\n# Visual\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=avg_salary.values,\n    y=avg_salary.index,\n    palette=\"crest\"\n)\nplt.title(\"Average Salary by Industry (Excluding Unclassified)\")\nplt.xlabel(\"Average Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\n\n# save\nplt.savefig(\"figures/average_salary_by_industry.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n/tmp/ipykernel_2618/1800265279.py:28: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n\n\n\n\n\n\n\n\n\n\n\n2.3 5.3 Remote vs. On-Site Jobs\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "",
    "text": "Our team chose Geographic and Remote Work Analysis as our research topic. Since the outbreak of the pandemic, the rise of remote work, along with the continuous emergence of opportunities related to artificial intelligence, has changed the way job seekers evaluate career choices. At the same time, cities like Austin, Denver and Raleigh-Durham are leveraging flexible work policies and lower living costs to attract new talent."
  },
  {
    "objectID": "index.html#research-rationale",
    "href": "index.html#research-rationale",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "",
    "text": "Our team chose Geographic and Remote Work Analysis as our research topic. Since the outbreak of the pandemic, the rise of remote work, along with the continuous emergence of opportunities related to artificial intelligence, has changed the way job seekers evaluate career choices. At the same time, cities like Austin, Denver and Raleigh-Durham are leveraging flexible work policies and lower living costs to attract new talent."
  },
  {
    "objectID": "index.html#why-is-this-topic-important",
    "href": "index.html#why-is-this-topic-important",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "Why is this topic important?",
    "text": "Why is this topic important?\nIn today’s AI-dominated landscape, geographic location and skill sets have become pivotal factors in employment opportunities. As AI-driven technologies continue to enhance productivity, understanding the following aspects is critical: comparative job growth between AI and traditional sectors, hiring dynamics in major tech hubs such as Silicon Valley, Boston, and Austin, the urban-rural divide in AI versus non-AI career opportunities, and the transformation of remote work paradigms."
  },
  {
    "objectID": "index.html#trend-analysis",
    "href": "index.html#trend-analysis",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "Trend Analysis",
    "text": "Trend Analysis\nThis section examines geographic and remote work dynamics in the 2024–2025 U.S. labor market.\nAI-related jobs are highly concentrated in tech and policy hubs. Washington, D.C. leads with 4.4% of postings requiring AI skills, followed by Washington State and Delaware. At the city level, New York, Seattle, and San Jose dominate in absolute AI job postings, while D.C. has the highest proportion. Nationally, postings mentioning AI skills rose 20% year-over-year in 2024, with generative AI skills nearly quadrupling (Lightcast and Stanford HAI (2025), 1–17). In contrast, non-AI job growth is broad-based: 33 states reported employment gains in 2024, led by Rochester, MN (+6.5%) and Stockton–Lodi, CA (+5.3%), driven by healthcare and logistics (U.S. Bureau of Labor Statistics (2023), 1–2).\nRemote work has stabilized at a “new normal.” Although job postings labeled remote declined from &gt;10% in 2022 to ~7.5% in May 2025 (Indeed Hiring Lab (2024)), telework rates remain far above pre-pandemic levels, with ~20% of workers telecommuting in 2024 (U.S. Bureau of Labor Statistics (2024), 12). By industry, remote adoption is highest in IT, finance, and management (Makridis (2024), 16; Bick, Blandin, and Mertens (2024), 7)), while healthcare, retail, and manufacturing remain predominantly on-site. Employers are shifting toward hybrid models, often requiring more in-office presence, though remote-capable roles continue to play a significant role (Barrero, Bloom, and Davis (2023), 20–21).\nTraditional tech hubs like Silicon Valley and New York City remain dominant in high-skill hiring, with over 65% of AI engineers located in these regions as of 2024. Despite rising costs and the expansion of remote work, their strong innovation ecosystems and dense talent networks preserve their competitive edge. At the same time, geographic diversification is accelerating: Miami and San Diego are emerging as fast-growing hubs, driven by lifestyle appeal, lower living costs, and, in Miami’s case, favorable tax policies. Miami saw a 12% increase in AI roles, while San Diego experienced a 7% rise in Big Tech hiring and raised $5.7 billion in venture capital. Conversely, former growth leaders like Austin and Houston are losing momentum, with startup employment declining by 6% and 10.9%, respectively, due to infrastructure gaps, cultural mismatch, and a shift back toward hybrid work models (SignalFire (2025)).\nGenerative AI introduces a reversal in the geography of job market exposure compared to earlier automation technologies. Whereas prior waves of automation—such as robotics and enterprise software—primarily disrupted rural and small-town economies by replacing routine manual or physical labor, generative AI disproportionately affects urban, high-skill, white-collar labor markets. These urban centers, including San Jose, San Francisco, New York, and Washington D.C., are home to occupations rich in cognitive, nonroutine tasks such as coding, writing, and data analysis—tasks highly susceptible to AI augmentation or displacement(Muro, Methkupally, and Kinder (2025)).\nAs a result, AI exposure rates in urban counties often exceed 40%, while rural counties average closer to 30%, reflecting both differing occupational structures and varying access to information-oriented industries. While rural areas may be more insulated from AI disruption, they are also less likely to benefit from AI-driven productivity gains. This spatial shift in exposure demands new policy attention focused on urban workforce reskilling, while also ensuring rural communities are not left behind in the emerging AI economy(Muro, Methkupally, and Kinder (2025))."
  },
  {
    "objectID": "index.html#what-do-you-expect-to-find-in-your-analysis",
    "href": "index.html#what-do-you-expect-to-find-in-your-analysis",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "What do you expect to find in your analysis?",
    "text": "What do you expect to find in your analysis?\nOur research encompasses various aspects of geographic distribution and remote work trends. Specifically, we aim to investigate: Job growth patterns across different cities and states, including a comparative analysis of which metropolitan areas demonstrate the highest expansion rates for both AI-related and non-AI career opportunities. The evolution of remote work arrangements, examining whether remote positions are experiencing growth or decline and how this shift affects workforce distribution. Comprehensive analysis of tech hub dynamics, identifying whether traditional technology centers like Silicon Valley, Austin, and Boston continue to dominate the hiring landscape, or if emerging locations are gaining competitive advantages. The urban-rural divide in employment opportunities, investigating how job market conditions differ between metropolitan and rural areas for both AI and traditional professional roles, and what this means for workforce mobility and regional economic development."
  }
]