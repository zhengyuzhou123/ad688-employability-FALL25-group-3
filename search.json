[
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Compute average salary by industry\navg_salary = (\n    df.groupby(industry_col)[salary_col]\n    .mean()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\n# Visual\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=avg_salary.values,\n    y=avg_salary.index,\n    orient=\"h\"\n)\nplt.title(\"Average Salary by Industry (Excluding Unclassified)\")\nplt.xlabel(\"Average Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\n\n# save\nplt.savefig(\"figures/average_salary_by_industry.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother bar chart was used to present the average salary across industries. Bar charts are effective for showing ranked values, making it easy to identify the highest-paying sectors and support data-driven career decisions.\nAverage pay skews toward specialized, capital-intensive niches. Polish & Other Sanitation Good Manufacturing tops the list (~$215k), followed by Rubber & Plastics Hoses/Belting and Agents for Wireless Telecommunications Services (~$185–$200k). A middle tier includes Irradiation Apparatus and Misc. Waste Management (~$175–$185k), while Web Search Portals, Plastics Bag/Pouch, and transport/retail (Rail Support, Electronics & Appliance Retailers, Wineries) cluster near the lower bound (~$150–$165k). Overall, excluding unclassified roles, high salaries concentrate in specialized manufacturing and regulated services.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "eda.html#exploratory-data-analysis-eda",
    "href": "eda.html#exploratory-data-analysis-eda",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Compute average salary by industry\navg_salary = (\n    df.groupby(industry_col)[salary_col]\n    .mean()\n    .sort_values(ascending=False)\n    .head(10)\n)\n\n# Visual\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=avg_salary.values,\n    y=avg_salary.index,\n    orient=\"h\"\n)\nplt.title(\"Average Salary by Industry (Excluding Unclassified)\")\nplt.xlabel(\"Average Salary ($)\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\n\n# save\nplt.savefig(\"figures/average_salary_by_industry.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nAnother bar chart was used to present the average salary across industries. Bar charts are effective for showing ranked values, making it easy to identify the highest-paying sectors and support data-driven career decisions.\nAverage pay skews toward specialized, capital-intensive niches. Polish & Other Sanitation Good Manufacturing tops the list (~$215k), followed by Rubber & Plastics Hoses/Belting and Agents for Wireless Telecommunications Services (~$185–$200k). A middle tier includes Irradiation Apparatus and Misc. Waste Management (~$175–$185k), while Web Search Portals, Plastics Bag/Pouch, and transport/retail (Rail Support, Electronics & Appliance Retailers, Wineries) cluster near the lower bound (~$150–$165k). Overall, excluding unclassified roles, high salaries concentrate in specialized manufacturing and regulated services.\n\n\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce."
  },
  {
    "objectID": "nlp_methods.html",
    "href": "nlp_methods.html",
    "title": "1.import and setup",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Download nltk assets\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\n# Load cleaned dataset\nlightcast_data = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\nlightcast_data[\"BODY\"] = lightcast_data[\"BODY\"].fillna(\"\")\n\n[nltk_data] Downloading package stopwords to /home/runner/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/runner/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /home/runner/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n\n2.Basic Text Cleaning\n\n# Lowercase\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY\"].str.lower()\n\n# Remove non-letters and extra spaces\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY_lower\"].apply(\n    lambda x: re.sub(r\"[^a-z\\s]\", \" \", x)\n)\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY_lower\"].apply(\n    lambda x: re.sub(r\"\\s+\", \" \", x).strip()\n)\n\n\n\n3.Remove filler words\n\nfiller_words = [\n    \"the\",\"is\",\"in\",\"and\",\"to\",\"of\",\"a\",\"for\",\"on\",\"with\",\"as\",\"by\",\"at\",\"an\",\n    \"be\",\"this\",\"that\",\"it\",\"from\",\"or\"\n]\n\npattern = r'\\b(?:' + \"|\".join(filler_words) + r')\\b'\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY_lower\"].str.replace(pattern, \" \", regex=True)\nlightcast_data[\"BODY_lower\"] = lightcast_data[\"BODY_lower\"].str.replace(r\"\\s+\", \" \", regex=True)\n\n# Word count\nlightcast_data[\"BODY_word_count\"] = lightcast_data[\"BODY_lower\"].apply(lambda x: len(x.split()))\n\n\n\n4.Lemmatization\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_lemmatize(text):\n    tokens = text.split()\n    tokens = [w for w in tokens if w not in stop_words]\n    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n    return \" \".join(tokens)\n\nlightcast_data[\"BODY_lemmatized\"] = lightcast_data[\"BODY_lower\"].apply(preprocess_lemmatize)\n\n\n\n5.TF-IDF Vectorization\n\ntfidf_vectorizer = TfidfVectorizer(max_features=10000)\ntfidf_matrix = tfidf_vectorizer.fit_transform(lightcast_data[\"BODY_lemmatized\"])\n\nfeature_names = tfidf_vectorizer.get_feature_names_out()\ntfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n\nall_terms = pd.DataFrame({\n    \"term\": feature_names,\n    \"importance\": tfidf_scores\n}).sort_values(\"importance\", ascending=False)\n\n\n\n6.Remove meaningless high-frequency job description words\n\ngeneric_terms = {\n    \"data\",\"experience\",\"business\",\"job\",\"work\",\"team\",\"solution\",\"management\",\"system\",\n    \"client\",\"process\",\"year\",\"project\",\"ability\",\"position\",\"time\",\"service\",\"information\",\n    \"description\",\"requirement\",\"employee\",\"technical\",\"benefit\",\"customer\",\"application\",\n    \"need\",\"required\",\"preferred\",\"must\",\"strong\",\"sap\",\"skill\",\"analyst\",\"support\",\"technology\",\n    \"opportunity\",\"analysis\",\"enterprise\", \"including\",\"role\",\"development\", \"status\",\"full\",\"program\",\"tool\",\"may\",\"product\",\"knowledge\",\"company\",\"report\",\"new\", \"working\",\"oracle\",\"functional\",\"related\",\"provide\",\"quality\",\"implementation\",\"id\",\"help\",\"day\",\"pay\",\"employer\",\"reporting\",\"based\",\"applicant\",\"detail\",\"hour\",\"employment\",\"design\", \"industry\",\"develop\",\"lead\",\"candidate\",\"ensure\",\"plan\",\"disability\", \"architecture\",\"range\",\n    \"degree\", \"level\",\"insight\",\"user\",\"practice\",\"career\",\"qualification\",\"erp\",\"paid\"\n}\n\nfiltered_terms = all_terms[~all_terms[\"term\"].isin(generic_terms)]\ntop_keywords = filtered_terms.head(20)\n\ntop_keywords\n\n\n\n\n\n\n\n\nterm\nimportance\n\n\n\n\n454\nanalytics\n1407.540441\n\n\n1640\ncloud\n1351.994428\n\n\n4730\ninsurance\n1335.000934\n\n\n4202\nhealth\n1225.704218\n\n\n5338\nlocation\n1153.550292\n\n\n8046\nsecurity\n1135.234112\n\n\n3163\nenvironment\n1014.948480\n\n\n8474\nsql\n1004.168204\n\n\n8529\nstate\n994.774118\n\n\n313\nai\n987.901993\n\n\n3607\nfinancial\n965.079915\n\n\n6644\nperformance\n939.683032\n\n\n1978\nconsultant\n938.917781\n\n\n1761\ncommunication\n929.598457\n\n\n124\nacross\n918.248186\n\n\n613\narchitect\n912.098399\n\n\n8601\nstrategy\n909.582766\n\n\n4742\nintegration\n901.683705\n\n\n5258\nlife\n888.971965\n\n\n6376\norganization\n888.029278\n\n\n\n\n\n\n\n\n\nCreate wordcloud\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Convert top keywords into a dictionary {term: importance}\nword_freq = dict(zip(top_keywords[\"term\"], top_keywords[\"importance\"]))\n\n# Create the word cloud\nwc = WordCloud(\n    width=1200,\n    height=600,\n    background_color=\"white\",\n    colormap=\"viridis\",   \n    prefer_horizontal=0.9\n).generate_from_frequencies(word_freq)\n\n# Plot it\nplt.figure(figsize=(14, 7))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Top NLP Keywords from Job Descriptions\", fontsize=18)\nplt.tight_layout()\n\n#save\nplt.savefig(\"figures/WordCloud.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "",
    "text": "1 Classification: Remote vs Non-Remote Jobs\n\n# Import necessary libraries\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Set plotly renderer\npio.renderers.default = \"notebook+notebook_connected+vscode\"\n\n# Initialize Spark Session\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", low_memory=False)\n\n# Print schema and preview first few rows\nprint(\"--- Diagnostic check: Schema and sample rows ---\")\nprint(df.info())\nprint(df.head())\n\n--- Diagnostic check: Schema and sample rows ---\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 72498 entries, 0 to 72497\nColumns: 131 entries, ID to NAICS_2022_6_NAME\ndtypes: float64(38), object(93)\nmemory usage: 72.5+ MB\nNone\n                                         ID LAST_UPDATED_DATE  \\\n0  1f57d95acf4dc67ed2819eb12f049f6a5c11782c          9/6/2024   \n1  0cb072af26757b6c4ea9464472a50a443af681ac          8/2/2024   \n2  85318b12b3331fa490d32ad014379df01855c557          9/6/2024   \n3  1b5c3941e54a1889ef4f8ae55b401a550708a310          9/6/2024   \n4  cb5ca25f02bdf25c13edfede7931508bfd9e858f         6/19/2024   \n\n      LAST_UPDATED_TIMESTAMP  DUPLICATES    POSTED    EXPIRED  DURATION  \\\n0  2024-09-06 20:32:57.352 Z         0.0  6/2/2024   6/8/2024       6.0   \n1  2024-08-02 17:08:58.838 Z         0.0  6/2/2024   8/1/2024       NaN   \n2  2024-09-06 20:32:57.352 Z         1.0  6/2/2024   7/7/2024      35.0   \n3  2024-09-06 20:32:57.352 Z         1.0  6/2/2024  7/20/2024      48.0   \n4  2024-06-19 07:00:00.000 Z         0.0  6/2/2024  6/17/2024      15.0   \n\n             SOURCE_TYPES                                        SOURCES  \\\n0       [\\n  \"Company\"\\n]                        [\\n  \"brassring.com\"\\n]   \n1     [\\n  \"Job Board\"\\n]                            [\\n  \"maine.gov\"\\n]   \n2     [\\n  \"Job Board\"\\n]                           [\\n  \"dejobs.org\"\\n]   \n3     [\\n  \"Job Board\"\\n]  [\\n  \"disabledperson.com\",\\n  \"dejobs.org\"\\n]   \n4  [\\n  \"FreeJobBoard\"\\n]                       [\\n  \"craigslist.org\"\\n]   \n\n                                                 URL  ... NAICS_2022_2  \\\n0  [\\n  \"https://sjobs.brassring.com/TGnewUI/Sear...  ...         44.0   \n1   [\\n  \"https://joblink.maine.gov/jobs/1085740\"\\n]  ...         56.0   \n2  [\\n  \"https://dejobs.org/dallas-tx/data-analys...  ...         52.0   \n3  [\\n  \"https://www.disabledperson.com/jobs/5948...  ...         52.0   \n4  [\\n  \"https://modesto.craigslist.org/sls/77475...  ...         99.0   \n\n                                   NAICS_2022_2_NAME NAICS_2022_3  \\\n0                                       Retail Trade        441.0   \n1  Administrative and Support and Waste Managemen...        561.0   \n2                              Finance and Insurance        524.0   \n3                              Finance and Insurance        522.0   \n4                              Unclassified Industry        999.0   \n\n                              NAICS_2022_3_NAME NAICS_2022_4  \\\n0               Motor Vehicle and Parts Dealers       4413.0   \n1           Administrative and Support Services       5613.0   \n2     Insurance Carriers and Related Activities       5242.0   \n3  Credit Intermediation and Related Activities       5221.0   \n4                         Unclassified Industry       9999.0   \n\n                                   NAICS_2022_4_NAME  NAICS_2022_5  \\\n0  Automotive Parts, Accessories, and Tire Retailers       44133.0   \n1                                Employment Services       56132.0   \n2  Agencies, Brokerages, and Other Insurance Rela...       52429.0   \n3                   Depository Credit Intermediation       52211.0   \n4                              Unclassified Industry       99999.0   \n\n                            NAICS_2022_5_NAME NAICS_2022_6  \\\n0  Automotive Parts and Accessories Retailers     441330.0   \n1                     Temporary Help Services     561320.0   \n2          Other Insurance Related Activities     524291.0   \n3                          Commercial Banking     522110.0   \n4                       Unclassified Industry     999999.0   \n\n                            NAICS_2022_6_NAME  \n0  Automotive Parts and Accessories Retailers  \n1                     Temporary Help Services  \n2                            Claims Adjusting  \n3                          Commercial Banking  \n4                       Unclassified Industry  \n\n[5 rows x 131 columns]\n\n\n\n# Take subset of relevant columns\nrelevant_columns = [\n    \"SALARY\", \"MIN_YEARS_EXPERIENCE\", \"EDUCATION_LEVELS_NAME\",\n    \"EMPLOYMENT_TYPE_NAME\", \"REMOTE_TYPE_NAME\", \"DURATION\", \n    \"IS_INTERNSHIP\", \"COMPANY_IS_STAFFING\", \"STATE_NAME\", \"CITY_NAME\",\n    \"MSA_NAME\", \"ONET\", \"ONET_NAME\", \"NAICS2_NAME\", \"TITLE_NAME\"\n]\n\ndf_analysis = df[relevant_columns].copy()\n\ndf_analysis.head()\n\n\n\n\n\n\n\n\nSALARY\nMIN_YEARS_EXPERIENCE\nEDUCATION_LEVELS_NAME\nEMPLOYMENT_TYPE_NAME\nREMOTE_TYPE_NAME\nDURATION\nIS_INTERNSHIP\nCOMPANY_IS_STAFFING\nSTATE_NAME\nCITY_NAME\nMSA_NAME\nONET\nONET_NAME\nNAICS2_NAME\nTITLE_NAME\n\n\n\n\n0\nNaN\n2.0\n[\\n \"Bachelor's degree\"\\n]\nFull-time (&gt; 32 hours)\n[None]\n6.0\nFalse\nFalse\nArkansas\nEl Dorado, AR\nEl Dorado, AR\n15-2051.01\nBusiness Intelligence Analysts\nRetail Trade\nEnterprise Analysts\n\n\n1\nNaN\n3.0\n[\\n \"No Education Listed\"\\n]\nFull-time (&gt; 32 hours)\nRemote\nNaN\nFalse\nTrue\nMaine\nAugusta, ME\nAugusta-Waterville, ME\n15-2051.01\nBusiness Intelligence Analysts\nAdministrative and Support and Waste Managemen...\nOracle Consultants\n\n\n2\nNaN\n5.0\n[\\n \"Bachelor's degree\"\\n]\nFull-time (&gt; 32 hours)\n[None]\n35.0\nFalse\nFalse\nTexas\nDallas, TX\nDallas-Fort Worth-Arlington, TX\n15-2051.01\nBusiness Intelligence Analysts\nFinance and Insurance\nData Analysts\n\n\n3\nNaN\n3.0\n[\\n \"No Education Listed\"\\n]\nFull-time (&gt; 32 hours)\n[None]\n48.0\nFalse\nFalse\nArizona\nPhoenix, AZ\nPhoenix-Mesa-Chandler, AZ\n15-2051.01\nBusiness Intelligence Analysts\nFinance and Insurance\nManagement Analysts\n\n\n4\n92500.0\nNaN\n[\\n \"No Education Listed\"\\n]\nPart-time / full-time\n[None]\n15.0\nFalse\nFalse\nCalifornia\nModesto, CA\nModesto, CA\n15-2051.01\nBusiness Intelligence Analysts\nUnclassified Industry\nUnclassified\n\n\n\n\n\n\n\n\ndf_analysis[\"REMOTE_TYPE_NAME\"] = df_analysis[\"REMOTE_TYPE_NAME\"].astype(str).str.strip()\n\nremote_col = df_analysis[\"REMOTE_TYPE_NAME\"].replace({\"[None]\": None})\n\ndf_analysis[\"REMOTE_GROUP\"] = np.select(\n    [\n        remote_col.eq(\"Remote\"),\n        remote_col.eq(\"Hybrid Remote\"),\n        remote_col.eq(\"Not Remote\"),\n        remote_col.isna()\n    ],\n    [\"Remote\", \"Hybrid\", \"Onsite\", \"Onsite\"],\n    default=\"Onsite\"\n)\n\ndf_analysis.drop(columns=[\"REMOTE_TYPE_NAME\"], inplace=True)\n\n\n# EMPLOYMENT_GROUP\nemp_col = df_analysis[\"EMPLOYMENT_TYPE_NAME\"].astype(str).str.strip()\n\ndf_analysis[\"EMPLOYMENT_GROUP\"] = np.select(\n    [\n        emp_col.eq(\"Full-time (&gt; 32 hours)\"),\n        emp_col.eq(\"Part-time (â¤ 32 hours)\"),\n        emp_col.eq(\"Part-time / full-time\"),\n        emp_col.isna()\n    ],\n    [\"Full-time\", \"Part-time\", \"Flexible\", \"Full-time\"],\n    default=\"Flexible\"\n)\n\ndf_analysis.drop(columns=[\"EMPLOYMENT_TYPE_NAME\"], inplace=True)\n\n# MIN_YEARS_EXPERIENCE & group\n\ndf_analysis[\"MIN_YEARS_EXPERIENCE\"] = df_analysis[\"MIN_YEARS_EXPERIENCE\"].fillna(0)\n\n# make sure it is numerical\ndf_analysis[\"MIN_YEARS_EXPERIENCE\"] = pd.to_numeric(\n    df_analysis[\"MIN_YEARS_EXPERIENCE\"],\n    errors=\"coerce\"\n).fillna(0)\n\nexp = df_analysis[\"MIN_YEARS_EXPERIENCE\"]\n\ndf_analysis[\"MIN_YEARS_EXPERIENCE_GROUP\"] = np.select(\n    [\n        (exp &gt;= 0) & (exp &lt;= 1),\n        (exp &gt; 1) & (exp &lt;= 3),\n        (exp &gt; 3) & (exp &lt;= 5),\n        (exp &gt; 5) & (exp &lt;= 10),\n        (exp &gt; 10)\n    ],\n    [\"Internship/Entry Level\", \"Junior\", \"Mid-Level\", \"Senior\", \"Expert\"],\n    default=\"Internship/Entry Level\"\n)\n\n\n# DURATION：null value & 0 -&gt; 1\n\ndur = df_analysis[\"DURATION\"]\ndf_analysis[\"DURATION\"] = (\n    dur.fillna(1)\n       .replace(0, 1)\n)\n\n\n# clean EDUCATION_LEVELS_NAME -&gt; EDUCATION_LEVELS_CLEAN\n\nedu_raw = df_analysis[\"EDUCATION_LEVELS_NAME\"].fillna(\"\")\nedu_clean = (\n    edu_raw\n    .astype(str)\n    .str.replace(r\"[\\[\\]\\n]\", \"\", regex=True)\n    .str.strip()\n)\ndf_analysis[\"EDUCATION_LEVELS_CLEAN\"] = edu_clean\ndf_analysis.drop(columns=[\"EDUCATION_LEVELS_NAME\"], inplace=True)\n\n\n# Fill in the blank STATE_NAME/CITY_NAME\n\n\ndf_analysis[\"STATE_NAME\"] = df_analysis[\"STATE_NAME\"].fillna(\"Unknown\")\ndf_analysis[\"CITY_NAME\"] = df_analysis[\"CITY_NAME\"].fillna(\"Unknown\")\n\n\n# ONET/ONET_NAME/NAICS2_NAME null value handling\n\ndf_analysis[\"ONET\"] = df_analysis[\"ONET\"].fillna(\"00-0000.00\")\ndf_analysis[\"ONET_NAME\"] = df_analysis[\"ONET_NAME\"].fillna(\"Unknown\")\ndf_analysis[\"NAICS2_NAME\"] = df_analysis[\"NAICS2_NAME\"].fillna(\"Unknown\")\n\nprint(df_analysis.head())\n\n    SALARY  MIN_YEARS_EXPERIENCE  DURATION IS_INTERNSHIP COMPANY_IS_STAFFING  \\\n0      NaN                   2.0       6.0         False               False   \n1      NaN                   3.0       1.0         False                True   \n2      NaN                   5.0      35.0         False               False   \n3      NaN                   3.0      48.0         False               False   \n4  92500.0                   0.0      15.0         False               False   \n\n   STATE_NAME      CITY_NAME                         MSA_NAME        ONET  \\\n0    Arkansas  El Dorado, AR                    El Dorado, AR  15-2051.01   \n1       Maine    Augusta, ME           Augusta-Waterville, ME  15-2051.01   \n2       Texas     Dallas, TX  Dallas-Fort Worth-Arlington, TX  15-2051.01   \n3     Arizona    Phoenix, AZ        Phoenix-Mesa-Chandler, AZ  15-2051.01   \n4  California    Modesto, CA                      Modesto, CA  15-2051.01   \n\n                        ONET_NAME  \\\n0  Business Intelligence Analysts   \n1  Business Intelligence Analysts   \n2  Business Intelligence Analysts   \n3  Business Intelligence Analysts   \n4  Business Intelligence Analysts   \n\n                                         NAICS2_NAME           TITLE_NAME  \\\n0                                       Retail Trade  Enterprise Analysts   \n1  Administrative and Support and Waste Managemen...   Oracle Consultants   \n2                              Finance and Insurance        Data Analysts   \n3                              Finance and Insurance  Management Analysts   \n4                              Unclassified Industry         Unclassified   \n\n  REMOTE_GROUP EMPLOYMENT_GROUP MIN_YEARS_EXPERIENCE_GROUP  \\\n0       Onsite        Full-time                     Junior   \n1       Remote        Full-time                     Junior   \n2       Onsite        Full-time                  Mid-Level   \n3       Onsite        Full-time                     Junior   \n4       Onsite         Flexible     Internship/Entry Level   \n\n  EDUCATION_LEVELS_CLEAN  \n0    \"Bachelor's degree\"  \n1  \"No Education Listed\"  \n2    \"Bachelor's degree\"  \n3  \"No Education Listed\"  \n4  \"No Education Listed\"  \n\n\n\n# Prepare binary classification data (Remote vs Onsite only)\n\ndf_binary = df_analysis[df_analysis[\"REMOTE_GROUP\"].isin([\"Remote\", \"Onsite\"])].copy()\n\nprint(df_binary[\"REMOTE_GROUP\"].value_counts())\n#\n\nREMOTE_GROUP\nOnsite    57741\nRemote    12497\nName: count, dtype: int64\n\n\n\n# Feature Engineering - Encode categorical variables\n\n\n# Define categorical and numeric columns\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ncategorical_cols = [\n    \"EMPLOYMENT_GROUP\",\n    \"MIN_YEARS_EXPERIENCE_GROUP\",\n    \"EDUCATION_LEVELS_CLEAN\",\n    \"STATE_NAME\",\n    \"NAICS2_NAME\"\n]\nnumeric_cols = [\"MIN_YEARS_EXPERIENCE\", \"DURATION\"]\n\n# 1. create label\n\nlabel_encoder = LabelEncoder()\ndf_binary[\"label\"] = label_encoder.fit_transform(df_binary[\"REMOTE_GROUP\"])\n\n# Take a look at the category order\nprint(\"Label mapping:\", dict(zip(label_encoder.classes_,\n                                 label_encoder.transform(label_encoder.classes_))))\n\n# 2. create features（indexer + onehot + VectorAssembler）\nX = df_binary[categorical_cols + numeric_cols]\ny = df_binary[\"label\"]\n\n# ColumnTransformer \npreprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols),\n    ]\n)\n\nX_features = preprocess.fit_transform(X)\n\nprint(\"--- Prepared Data Preview (pandas + sklearn) ---\")\nprint(\"X_features type:\", type(X_features))\nprint(\"X_features shape:\", X_features.shape)\nprint(\"First 5 labels:\", y.iloc[:5].tolist())\nprint(\"First 5 REMOTE_GROUP:\", df_binary[\"REMOTE_GROUP\"].iloc[:5].tolist())\n\n\nif hasattr(X_features, \"toarray\"):\n    X_dense = X_features.toarray()\nelse:\n    X_dense = np.asarray(X_features)\n\n\ndf_prepared = pd.DataFrame({\n    \"REMOTE_GROUP\": df_binary[\"REMOTE_GROUP\"].reset_index(drop=True),\n    \"label\": y.reset_index(drop=True),     # df_binary[\"label\"]\n    \"features\": list(X_dense)             # One for each line numpy array\n})\n\nprint(\"\\n--- df_prepared preview ---\")\nprint(df_prepared.head())\nprint(df_prepared.shape)\n\nLabel mapping: {'Onsite': np.int64(0), 'Remote': np.int64(1)}\n--- Prepared Data Preview (pandas + sklearn) ---\nX_features type: &lt;class 'scipy.sparse._csr.csr_matrix'&gt;\nX_features shape: (70238, 112)\nFirst 5 labels: [0, 1, 0, 0, 0]\nFirst 5 REMOTE_GROUP: ['Onsite', 'Remote', 'Onsite', 'Onsite', 'Onsite']\n\n--- df_prepared preview ---\n  REMOTE_GROUP  label                                           features\n0       Onsite      0  [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n1       Remote      1  [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n2       Onsite      0  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...\n3       Onsite      0  [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n4       Onsite      0  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n(70238, 3)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_data, test_data = train_test_split(\n    df_prepared,\n    test_size=0.3,        # 30% for test\n    random_state=42,      # seed=42\n)\n\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Test set size: {len(test_data)}\")\n\nTraining set size: 49166\nTest set size: 21072\n\n\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# break out X / y\nX_train = np.stack(train_data[\"features\"].to_numpy())   # shape: (n_train, n_features)\ny_train = train_data[\"label\"].to_numpy()\n\nX_test  = np.stack(test_data[\"features\"].to_numpy())\ny_test  = test_data[\"label\"].to_numpy()\n\n# Logistic Regression\nlr_model = LogisticRegression(\n    max_iter=1000,      \n    n_jobs=-1\n)\nlr_model.fit(X_train, y_train)\n\n# Random Forest\nrf_model = RandomForestClassifier(\n    n_estimators=100,   # = numTrees\n    random_state=42,    # = seed\n    n_jobs=-1\n)\nrf_model.fit(X_train, y_train)\n\nprint(\"Both models trained successfully!\")\n\nBoth models trained successfully!\n\n\n\n\n2 Predict\n\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\n\n# First extract X_test/y_test from test_data\nX_test = np.stack(test_data[\"features\"].to_numpy())\ny_test = test_data[\"label\"].to_numpy()\n\n\n# Predict\n\n# Logistic Regression\ny_pred_lr = lr_model.predict(X_test)\n\n# Obtain the positive class probability using predict_proba and calculate the AUC\ny_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n\n# Random Forest\ny_pred_rf = rf_model.predict(X_test)\ny_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n\n\n# Evaluation：Accuracy / F1 / AUC-ROC\nfrom sklearn.metrics import f1_score\n\nprint(\"LOGISTIC REGRESSION RESULTS\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\nprint(f\"F1 Score (weighted): {f1_score(y_test, y_pred_lr, average='weighted'):.4f}\")\nprint(f\"AUC-ROC:  {roc_auc_score(y_test, y_proba_lr):.4f}\")\n\nprint(\"RANDOM FOREST RESULTS\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\nprint(f\"F1 Score (weighted): {f1_score(y_test, y_pred_rf, average='weighted'):.4f}\")\nprint(f\"AUC-ROC:  {roc_auc_score(y_test, y_proba_rf):.4f}\")\n\nLOGISTIC REGRESSION RESULTS\nAccuracy: 0.8250\nF1 Score (weighted): 0.7533\nAUC-ROC:  0.6368\nRANDOM FOREST RESULTS\nAccuracy: 0.8349\nF1 Score (weighted): 0.8168\nAUC-ROC:  0.7293\n\n\n\n# Step 7: Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\nprint(\"\\n=== Logistic Regression Confusion Matrix (2x2) ===\")\nprint(\"(rows = true label, cols = predicted label)\")\n\ncm_lr = confusion_matrix(y_test, y_pred_lr, labels=[0, 1])\ncm_lr_df = pd.DataFrame(\n    cm_lr,\n    index=[\"True 0 (Onsite)\", \"True 1 (Remote)\"],\n    columns=[\"Pred 0 (Onsite)\", \"Pred 1 (Remote)\"]\n)\nprint(cm_lr_df)\n\nprint(\"\\n=== Random Forest Confusion Matrix (2x2) ===\")\ncm_rf = confusion_matrix(y_test, y_pred_rf, labels=[0, 1])\ncm_rf_df = pd.DataFrame(\n    cm_rf,\n    index=[\"True 0 (Onsite)\", \"True 1 (Remote)\"],\n    columns=[\"Pred 0 (Onsite)\", \"Pred 1 (Remote)\"]\n)\nprint(cm_rf_df)\n\n\n=== Logistic Regression Confusion Matrix (2x2) ===\n(rows = true label, cols = predicted label)\n                 Pred 0 (Onsite)  Pred 1 (Remote)\nTrue 0 (Onsite)            17274               59\nTrue 1 (Remote)             3628              111\n\n=== Random Forest Confusion Matrix (2x2) ===\n                 Pred 0 (Onsite)  Pred 1 (Remote)\nTrue 0 (Onsite)            16372              961\nTrue 1 (Remote)             2517             1222\n\n\n\n\n3 confusion matrix\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\n# Calculate the confusion matrix (in the order of [0, 1])\ncm_lr = confusion_matrix(y_test, y_pred_lr, labels=[0, 1])\ncm_rf = confusion_matrix(y_test, y_pred_rf, labels=[0, 1])\n\nprint(\"Logistic Regression CM:\\n\", cm_lr)\nprint(\"Random Forest CM:\\n\", cm_rf)\n\n# The z (list of list) required for converting to plotly\nlr_z = cm_lr.tolist()\nrf_z = cm_rf.tolist()\n\nlr_x = ['Onsite', 'Remote']  # Predicted label\nlr_y = ['Onsite', 'Remote']  # True label\n\n# Visual\nfig = make_subplots(\n    rows=1, cols=2, \n    subplot_titles=('Logistic Regression', 'Random Forest')\n)\n\n# Logistic Regression heatmap\nfig.add_trace(\n    go.Heatmap(\n        z=lr_z,\n        x=lr_x,\n        y=lr_y,\n        colorscale='Blues',\n        text=lr_z,\n        texttemplate=\"%{text}\",\n        showscale=False\n    ),\n    row=1, col=1\n)\n\n# Random Forest heatmap\nfig.add_trace(\n    go.Heatmap(\n        z=rf_z,\n        x=lr_x,\n        y=lr_y,\n        colorscale='Blues',\n        text=rf_z,\n        texttemplate=\"%{text}\",\n        showscale=True\n    ),\n    row=1, col=2\n)\n\nfig.update_layout(\n    title_text=\"Remote vs Onsite Job Classification - Confusion Matrix Comparison\",\n    title_font_size=16,\n    height=400,\n    width=900\n)\n\nfig.update_xaxes(title_text=\"Predicted Label\")\nfig.update_yaxes(title_text=\"True Label\")\n\nfig.write_image(\"figures/confusion_matrix_comparison.jpg\")\n\nLogistic Regression CM:\n [[17274    59]\n [ 3628   111]]\nRandom Forest CM:\n [[16372   961]\n [ 2517  1222]]\n\n\n\n\n4 KMeans Clustering using ONET as reference label\n\n# KMeans Clustering using ONET as reference label\n\n# Check ONET distribution first\nonet_dist_df = (\n    df_analysis[\"ONET_NAME\"]\n    .value_counts()\n    .reset_index()\n    .rename(columns={\"index\": \"ONET_NAME\", \"ONET_NAME\": \"count\"})\n)\n\nprint(onet_dist_df.head(20))\n\n                            count  count\n0  Business Intelligence Analysts  72454\n1                         Unknown     44\n\n\n\n\n5 Prepare features for KMeans clustering\n\n# Prepare features for KMeans clustering\n# Define columns\ncluster_categorical_cols = [\n    \"EMPLOYMENT_GROUP\",\n    \"MIN_YEARS_EXPERIENCE_GROUP\",\n    \"EDUCATION_LEVELS_CLEAN\",\n    \"STATE_NAME\",\n    \"REMOTE_GROUP\",\n]\ncluster_numeric_cols = [\"MIN_YEARS_EXPERIENCE\", \"DURATION\"]\n\n# Prepare the feature table X (only including the columns used for clustering)\nX_cluster = df_analysis[cluster_categorical_cols + cluster_numeric_cols]\n\n# 3. ColumnTransformer = StringIndexer + OneHotEncoder + VectorAssembler\ncluster_preprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cluster_categorical_cols),\n        (\"num\", \"passthrough\", cluster_numeric_cols),\n    ]\n)\n\ncluster_pipeline = Pipeline(\n    steps=[\n        (\"preprocess\", cluster_preprocess)\n    ]\n)\n\n# Fit and merge to generate the features matrix\nX_cluster_features = cluster_pipeline.fit_transform(X_cluster)\n\nif hasattr(X_cluster_features, \"toarray\"):\n    X_cluster_dense = X_cluster_features.toarray()\nelse:\n    X_cluster_dense = np.asarray(X_cluster_features)\n\nprint(\"--- Clustering Features Shape ---\")\nprint(X_cluster_dense.shape)\n\n# 5. ONET_NAME -&gt; ONET_label\nonet_le = LabelEncoder()\ndf_analysis = df_analysis.copy()\ndf_analysis[\"ONET_NAME\"] = df_analysis[\"ONET_NAME\"].fillna(\"Unknown\")\ndf_analysis[\"ONET_label\"] = onet_le.fit_transform(df_analysis[\"ONET_NAME\"])\n\nprint(\"ONET label mapping (first few):\")\nfor name, idx in list(zip(onet_le.classes_, range(len(onet_le.classes_))))[:10]:\n    print(idx, \"-&gt;\", name)\n\n# 6. group df_cluster：ONET_NAME | ONET_label | features\ndf_cluster = df_analysis.copy()\ndf_cluster[\"features\"] = list(X_cluster_dense)\n\nprint(\"--- Clustering Data Prepared (pandas) ---\")\nprint(df_cluster[[\"ONET_NAME\", \"ONET_label\", \"features\"]].head())\nprint(df_cluster.shape)\n\n--- Clustering Features Shape ---\n(72498, 94)\nONET label mapping (first few):\n0 -&gt; Business Intelligence Analysts\n1 -&gt; Unknown\n--- Clustering Data Prepared (pandas) ---\n                        ONET_NAME  ONET_label  \\\n0  Business Intelligence Analysts           0   \n1  Business Intelligence Analysts           0   \n2  Business Intelligence Analysts           0   \n3  Business Intelligence Analysts           0   \n4  Business Intelligence Analysts           0   \n\n                                            features  \n0  [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n1  [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n2  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n3  [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n4  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n(72498, 18)\n\n\n\n\n6 Find optimal K using Elbow Method\n\n# Find optimal K using Elbow Method\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Extract the feature matrix from the df_cluster\nX_cluster = np.stack(df_cluster[\"features\"].to_numpy())   # shape: (n_samples, n_features)\n\nk_values = [3, 5, 7, 10, 15, 20]\nsilhouette_scores = []\n\nprint(\"--- Finding Optimal K ---\")\nfor k in k_values:\n    kmeans = KMeans(\n        n_clusters=k,\n        random_state=42,    # seed = 42\n        n_init=\"auto\"      \n    )\n    labels = kmeans.fit_predict(X_cluster)\n\n    score = silhouette_score(X_cluster, labels)\n    silhouette_scores.append(score)\n\n    print(f\"K = {k}: Silhouette Score = {score:.4f}\")\n\n--- Finding Optimal K ---\nK = 3: Silhouette Score = 0.5132\nK = 5: Silhouette Score = 0.4411\nK = 7: Silhouette Score = 0.3804\nK = 10: Silhouette Score = 0.3590\nK = 15: Silhouette Score = 0.3334\nK = 20: Silhouette Score = 0.2926\n\n\n\n\n7 Extract the feature matrix\n\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Extract the feature matrix\nX_cluster = np.stack(df_cluster[\"features\"].to_numpy())  # shape: (n_samples, n_features)\n\n# Train the final KMeans model\noptimal_k = 3\n\nkmeans_final = KMeans(\n    n_clusters=optimal_k,\n    random_state=42,   # seed = 42\n    n_init=\"auto\"\n)\n\ncluster_labels = kmeans_final.fit_predict(X_cluster)  \n\n# Write the cluster back to the DataFrame\ndf_clustered = df_cluster.copy()\ndf_clustered[\"cluster\"] = cluster_labels   \n\n# 4. calculate silhouette\nfinal_score = silhouette_score(X_cluster, cluster_labels)\nprint(f\"\\n--- Final KMeans Model (K={optimal_k}) ---\")\nprint(f\"Silhouette Score: {final_score:.4f}\")\n\n# Cluster Distribution\nprint(\"\\n--- Cluster Distribution ---\")\nprint(\n    df_clustered[\"cluster\"]\n    .value_counts()\n    .sort_index()\n)\n\n\n--- Final KMeans Model (K=3) ---\nSilhouette Score: 0.5132\n\n--- Cluster Distribution ---\ncluster\n0    19473\n1    39891\n2    13134\nName: count, dtype: int64\n\n\n\n\n8 Analyze clusters vs ONET reference labels\n\n# Cross-tabulation of clusters and ONET\nprint(\"--- Top ONET categories in each cluster ---\")\nfor cluster_id in range(optimal_k):\n    print(f\"\\n=== Cluster {cluster_id} ===\")\n    top_onet_df = (\n        df_clustered[df_clustered[\"cluster\"] == cluster_id][\"ONET_NAME\"]\n        .value_counts()\n        .reset_index()\n        .rename(columns={\"index\": \"ONET_NAME\", \"ONET_NAME\": \"count\"})\n        .head(5)\n    )\n    print(top_onet_df)\n\n--- Top ONET categories in each cluster ---\n\n=== Cluster 0 ===\n                            count  count\n0  Business Intelligence Analysts  19472\n1                         Unknown      1\n\n=== Cluster 1 ===\n                            count  count\n0  Business Intelligence Analysts  39851\n1                         Unknown     40\n\n=== Cluster 2 ===\n                            count  count\n0  Business Intelligence Analysts  13131\n1                         Unknown      3\n\n\n\n\n9 Analyze cluster characteristics\n\nprint(\"--- Cluster Characteristics ---\")\n\n# === Remote work distribution by cluster ===\nprint(\"\\n=== Remote Work by Cluster ===\")\nremote_by_cluster = (\n    df_clustered\n    .groupby([\"cluster\", \"REMOTE_GROUP\"])\n    .size()\n    .reset_index(name=\"count\")\n    .sort_values([\"cluster\", \"REMOTE_GROUP\"])\n)\nprint(remote_by_cluster)\n\nremote_pivot = remote_by_cluster.pivot(\n    index=\"cluster\",\n    columns=\"REMOTE_GROUP\",\n    values=\"count\"\n).fillna(0).astype(int)\nprint(\"\\nRemote work pivot table:\")\nprint(remote_pivot)\n\n# === Experience level by cluster ===\nprint(\"\\n=== Experience Level by Cluster ===\")\nexp_by_cluster = (\n    df_clustered\n    .groupby([\"cluster\", \"MIN_YEARS_EXPERIENCE_GROUP\"])\n    .size()\n    .reset_index(name=\"count\")\n    .sort_values([\"cluster\", \"MIN_YEARS_EXPERIENCE_GROUP\"])\n)\nprint(exp_by_cluster)\n\nexp_pivot = exp_by_cluster.pivot(\n    index=\"cluster\",\n    columns=\"MIN_YEARS_EXPERIENCE_GROUP\",\n    values=\"count\"\n).fillna(0).astype(int)\nprint(\"\\nExperience level pivot table:\")\nprint(exp_pivot)\n\n# === Top states by cluster ===\nprint(\"\\n=== Top States by Cluster ===\")\nfor cluster_id in range(optimal_k):\n    print(f\"\\n--- Cluster {cluster_id} Top States ---\")\n    top_states = (\n        df_clustered[df_clustered[\"cluster\"] == cluster_id][\"STATE_NAME\"]\n        .value_counts()\n        .head(5)\n    )\n    print(top_states)\n\n--- Cluster Characteristics ---\n\n=== Remote Work by Cluster ===\n   cluster REMOTE_GROUP  count\n0        0       Hybrid    540\n1        0       Onsite  15203\n2        0       Remote   3730\n3        1       Hybrid   1177\n4        1       Onsite  32518\n5        1       Remote   6196\n6        2       Hybrid    543\n7        2       Onsite  10020\n8        2       Remote   2571\n\nRemote work pivot table:\nREMOTE_GROUP  Hybrid  Onsite  Remote\ncluster                             \n0                540   15203    3730\n1               1177   32518    6196\n2                543   10020    2571\n\n=== Experience Level by Cluster ===\n    cluster MIN_YEARS_EXPERIENCE_GROUP  count\n0         0                     Expert    922\n1         0     Internship/Entry Level   7249\n2         0                     Junior   3628\n3         0                  Mid-Level   3894\n4         0                     Senior   3780\n5         1                     Expert   1847\n6         1     Internship/Entry Level  14474\n7         1                     Junior   7179\n8         1                  Mid-Level   7681\n9         1                     Senior   8710\n10        2                     Expert    705\n11        2     Internship/Entry Level   4953\n12        2                     Junior   2438\n13        2                  Mid-Level   2348\n14        2                     Senior   2690\n\nExperience level pivot table:\nMIN_YEARS_EXPERIENCE_GROUP  Expert  Internship/Entry Level  Junior  Mid-Level  \\\ncluster                                                                         \n0                              922                    7249    3628       3894   \n1                             1847                   14474    7179       7681   \n2                              705                    4953    2438       2348   \n\nMIN_YEARS_EXPERIENCE_GROUP  Senior  \ncluster                             \n0                             3780  \n1                             8710  \n2                             2690  \n\n=== Top States by Cluster ===\n\n--- Cluster 0 Top States ---\nSTATE_NAME\nTexas         2098\nCalifornia    2041\nFlorida       1069\nVirginia      1000\nNew York       801\nName: count, dtype: int64\n\n--- Cluster 1 Top States ---\nSTATE_NAME\nTexas         4519\nCalifornia    3839\nIllinois      2417\nVirginia      2082\nNew York      1969\nName: count, dtype: int64\n\n--- Cluster 2 Top States ---\nSTATE_NAME\nTexas         1450\nCalifornia    1204\nFlorida        636\nNew York       571\nVirginia       554\nName: count, dtype: int64\n\n\n\n\n10 Visualization: Elbow Plot\n\nimport pandas as pd\nimport plotly.express as px\n\n# obtain the cluster frequency\ncluster_counts_series = (\n    df_clustered[\"cluster\"]\n    .value_counts()\n    .sort_index()\n)\n\n# Clearly create a DataFrame with only two columns, \"cluster\" and \"count\"\ncluster_counts = pd.DataFrame({\n    \"cluster\": cluster_counts_series.index,\n    \"count\": cluster_counts_series.values\n})\n\nprint(cluster_counts)  # Check if the column names are ['cluster', 'count']\n\n# Visual\nfig = px.bar(\n    cluster_counts,\n    x=\"cluster\",\n    y=\"count\",\n    title=\"KMeans Clustering: Distribution of Jobs Across Clusters\",\n    labels={\"cluster\": \"Cluster\", \"count\": \"Number of Jobs\"},\n    template=\"plotly_white\",\n    color=\"count\",\n    color_continuous_scale=\"Blues\",\n)\n\nfig.update_layout(font=dict(family=\"Roboto\", size=14))\n\nfig.write_image(\"figures/kmeans_elbow_plot.jpg\")\nfig\n\n   cluster  count\n0        0  19473\n1        1  39891\n2        2  13134\n\n\n                            \n                                            \n\n\n\n\n11 Visualization: Cluster Distribution\n\nimport pandas as pd\nimport plotly.express as px\n\n# Visualization: Cluster Distribution\n\n# Get cluster counts（\nmapping = {0: 2, 1: 0, 2: 1}\n\ndf_clustered[\"cluster_spark\"] = df_clustered[\"cluster\"].map(mapping)\n\ncluster_counts = (\n    df_clustered[\"cluster_spark\"]\n    .value_counts()\n    .sort_index()\n)\n\n\nfig = px.bar(\n    x=cluster_counts.index,\n    y=cluster_counts.values,\n    color=cluster_counts.values,          \n    color_continuous_scale=\"Blues\",      \n    labels={\"x\": \"Cluster\", \"y\": \"Number of Jobs\", \"color\": \"Number of Jobs\"},\n    title=\"KMeans Clustering: Distribution of Jobs Across Clusters\",\n    template=\"plotly_white\",\n)\n\nfig.update_layout(font=dict(family=\"Roboto\", size=14))\n\nfig.write_image(\"figures/kmeans_cluster_distribution.jpg\")\nfig\n\n                            \n                                            \n\n\n\n\n12 Choropleth Map: Remote Work Percentage by State\n\nimport pandas as pd\nimport numpy as np\n\n# Calculate remote work percentage by state (pandas version)\n\n# 1.STATE_NAME & REMOTE_GROUP\nstate_remote = (\n    df_analysis\n    .groupby([\"STATE_NAME\", \"REMOTE_GROUP\"])\n    .size()\n    .reset_index(name=\"count\")\n)\n\n# 2. pivot：index = STATE_NAME，columns = REMOTE_GROUP（Onsite / Remote / Hybrid）\nstate_df = (\n    state_remote\n    .pivot(index=\"STATE_NAME\", columns=\"REMOTE_GROUP\", values=\"count\")\n    .fillna(0)\n    .reset_index()\n)\n\n# Ensure that the \"Hybrid\" column exists\nif \"Hybrid\" not in state_df.columns:\n    state_df[\"Hybrid\"] = 0\n\n# Calculate Total and Remote_Pct\nstate_df[\"Total\"] = state_df[\"Onsite\"] + state_df[\"Remote\"] + state_df[\"Hybrid\"]\nstate_df[\"Remote_Pct\"] = (state_df[\"Remote\"] / state_df[\"Total\"] * 100).round(2)\n\nprint(\"--- State Remote Work Data ---\")\nprint(state_df.head(10))\n\n--- State Remote Work Data ---\nREMOTE_GROUP   STATE_NAME  Hybrid  Onsite  Remote   Total  Remote_Pct\n0                 Alabama    15.0   567.0   108.0   690.0       15.65\n1                  Alaska     7.0   132.0    97.0   236.0       41.10\n2                 Arizona    43.0  1349.0   246.0  1638.0       15.02\n3                Arkansas    12.0   464.0   108.0   584.0       18.49\n4              California   262.0  5766.0  1056.0  7084.0       14.91\n5                Colorado    55.0  1141.0   259.0  1455.0       17.80\n6             Connecticut    41.0   660.0   162.0   863.0       18.77\n7                Delaware    15.0   330.0    93.0   438.0       21.23\n8                 Florida    72.0  3060.0   513.0  3645.0       14.07\n9                 Georgia    64.0  2169.0   425.0  2658.0       15.99\n\n\n\n\n13 Add state abbreviations for Plotly map\n\n# State name to abbreviation mapping\nstate_abbrev = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY',\n    'District of Columbia': 'DC'\n}\n\n# Add state abbreviation column\nstate_df['State_Abbrev'] = state_df['STATE_NAME'].map(state_abbrev)\n\n# Remove rows without valid state abbreviation (e.g., \"Unknown\")\nstate_df_clean = state_df[state_df['State_Abbrev'].notna()]\n\nprint(f\"States with data: {len(state_df_clean)}\")\nprint(state_df_clean[['STATE_NAME', 'State_Abbrev', 'Total', 'Remote_Pct']].head(10))\n\nStates with data: 50\nREMOTE_GROUP   STATE_NAME State_Abbrev   Total  Remote_Pct\n0                 Alabama           AL   690.0       15.65\n1                  Alaska           AK   236.0       41.10\n2                 Arizona           AZ  1638.0       15.02\n3                Arkansas           AR   584.0       18.49\n4              California           CA  7084.0       14.91\n5                Colorado           CO  1455.0       17.80\n6             Connecticut           CT   863.0       18.77\n7                Delaware           DE   438.0       21.23\n8                 Florida           FL  3645.0       14.07\n9                 Georgia           GA  2658.0       15.99\n\n\n\n\n14 Choropleth Map with State Labels showing Remote Percentage\n\nimport plotly.graph_objects as go\n\n# Choropleth Map with State Labels showing Remote Percentage\n\nfig = go.Figure(data=go.Choropleth(\n    locations=state_df_clean['State_Abbrev'],\n    z=state_df_clean['Remote_Pct'],\n    locationmode='USA-states',\n    colorscale='Blues',\n    colorbar_title='Remote %',\n    text=state_df_clean['State_Abbrev'] + '&lt;br&gt;' + state_df_clean['Remote_Pct'].astype(str) + '%',\n    hovertemplate='&lt;b&gt;%{text}&lt;/b&gt;&lt;br&gt;Total Jobs: %{customdata[0]}&lt;br&gt;Remote Jobs: %{customdata[1]}&lt;extra&gt;&lt;/extra&gt;',\n    customdata=state_df_clean[['Total', 'Remote']].values,\n    marker_line_color='white',\n    marker_line_width=1\n))\n\n# Add state abbreviations with percentages as annotations\nfig.add_scattergeo(\n    locations=state_df_clean['State_Abbrev'],\n    locationmode='USA-states',\n    text=state_df_clean['Remote_Pct'].apply(lambda x: f'{x:.0f}%'),\n    mode='text',\n    textfont=dict(size=8, color='black', family='Arial Black'),\n    showlegend=False\n)\n\nfig.update_layout(\n    title_text='Remote Work Opportunity by State (% of Jobs that are Remote)',\n    title_font_size=18,\n    geo=dict(\n        scope='usa',\n        projection_type='albers usa',\n        showlakes=True,\n        lakecolor='rgb(255, 255, 255)',\n        bgcolor='rgba(0,0,0,0)'\n    ),\n    template='plotly_white',\n    font=dict(family=\"Roboto\", size=14),\n    height=600,\n    width=1000\n)\n\nfig.write_image(\"figures/choropleth_remote_work_with_labels.jpg\")\nprint(\"Enhanced choropleth map saved!\")\nfig\n\nEnhanced choropleth map saved!\n\n\n                            \n                                            \n\n\n\n\n15 Choropleth Map: Total Job Postings by State (with labels)\n\nimport plotly.graph_objects as go\n\n# Choropleth Map: Total Job Postings by State (with labels)\n\n# Get top 15 states by total jobs for labeling\ntop_states = state_df_clean.nlargest(15, 'Total')['State_Abbrev'].tolist()\n\nfig = go.Figure(data=go.Choropleth(\n    locations=state_df_clean['State_Abbrev'],\n    z=state_df_clean['Total'],\n    locationmode='USA-states',\n    colorscale='Greens',\n    colorbar_title='Total Jobs',\n    hovertemplate='&lt;b&gt;%{location}&lt;/b&gt;&lt;br&gt;Total Jobs: %{z:,}&lt;br&gt;Remote: %{customdata[0]:,}&lt;br&gt;Onsite: %{customdata[1]:,}&lt;extra&gt;&lt;/extra&gt;',\n    customdata=state_df_clean[['Remote', 'Onsite']].values,\n    marker_line_color='white',\n    marker_line_width=1.5\n))\n\n# Add labels for top states (format large numbers with K)\ntop_state_df = state_df_clean[state_df_clean['State_Abbrev'].isin(top_states)].copy()\ntop_state_df['Total_Label'] = top_state_df['Total'].apply(\n    lambda x: f'{x/1000:.1f}K' if x &gt;= 1000 else str(int(x))\n)\n\nfig.add_scattergeo(\n    locations=top_state_df['State_Abbrev'],\n    locationmode='USA-states',\n    text=top_state_df['Total_Label'],\n    mode='text',\n    textfont=dict(size=10, color='darkgreen', family='Arial Black'),\n    showlegend=False\n)\n\nfig.update_layout(\n    title_text='Total Job Postings by State&lt;br&gt;&lt;sup&gt;Labels shown for top 15 states by job volume&lt;/sup&gt;',\n    title_font_size=16,\n    geo=dict(\n        scope='usa',\n        projection_type='albers usa',\n        showlakes=True,\n        lakecolor='rgb(255, 255, 255)'\n    ),\n    template='plotly_white',\n    font=dict(family=\"Roboto\", size=14),\n    height=600,\n    width=1000\n)\n\n\nfig.write_image(\"figures/choropleth_total_jobs_with_labels.jpg\")\nprint(\"Enhanced total jobs choropleth map saved!\")\nfig\n\nEnhanced total jobs choropleth map saved!\n\n\n                            \n                                            \n\n\n\n\n16 Bar Chart: Top 10 States by Remote Work Percentage\n\n# Filter states with at least 100 jobs for meaningful comparison\nstate_df_filtered = state_df_clean[state_df_clean['Total'] &gt;= 100]\n\n# Sort by remote percentage\ntop_remote_states = state_df_filtered.nlargest(10, 'Remote_Pct')\n\nfig = px.bar(\n    top_remote_states,\n    x='STATE_NAME',\n    y='Remote_Pct',\n    color='Remote_Pct',\n    color_continuous_scale='Blues',\n    title='Top 10 States with Highest Remote Work Opportunities',\n    labels={'STATE_NAME': 'State', 'Remote_Pct': 'Remote Work %'},\n    text='Remote_Pct'\n)\n\nfig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')\nfig.update_layout(\n    template='plotly_white',\n    font=dict(family=\"Roboto\", size=14),\n    xaxis_tickangle=-45,\n    showlegend=False\n)\n\nfig.write_image(\"figures/top10_remote_states.jpg\")\nprint(\"Top 10 remote states bar chart saved!\")\nfig\n\nTop 10 remote states bar chart saved!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "",
    "text": "Our team chose Geographic and Remote Work Analysis as our research topic. Since the outbreak of the pandemic, the rise of remote work, along with the continuous emergence of opportunities related to artificial intelligence, has changed the way job seekers evaluate career choices. At the same time, cities like Austin, Denver and Raleigh-Durham are leveraging flexible work policies and lower living costs to attract new talent."
  },
  {
    "objectID": "index.html#research-rationale",
    "href": "index.html#research-rationale",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "",
    "text": "Our team chose Geographic and Remote Work Analysis as our research topic. Since the outbreak of the pandemic, the rise of remote work, along with the continuous emergence of opportunities related to artificial intelligence, has changed the way job seekers evaluate career choices. At the same time, cities like Austin, Denver and Raleigh-Durham are leveraging flexible work policies and lower living costs to attract new talent."
  },
  {
    "objectID": "index.html#why-is-this-topic-important",
    "href": "index.html#why-is-this-topic-important",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "Why is this topic important?",
    "text": "Why is this topic important?\nIn today’s AI-dominated landscape, geographic location and skill sets have become pivotal factors in employment opportunities. As AI-driven technologies continue to enhance productivity, understanding the following aspects is critical: comparative job growth between AI and traditional sectors, hiring dynamics in major tech hubs such as Silicon Valley, Boston, and Austin, the urban-rural divide in AI versus non-AI career opportunities, and the transformation of remote work paradigms."
  },
  {
    "objectID": "index.html#trend-analysis",
    "href": "index.html#trend-analysis",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "Trend Analysis",
    "text": "Trend Analysis\nThis section examines geographic and remote work dynamics in the 2024–2025 U.S. labor market.\nAI-related jobs are highly concentrated in tech and policy hubs. Washington, D.C. leads with 4.4% of postings requiring AI skills, followed by Washington State and Delaware. At the city level, New York, Seattle, and San Jose dominate in absolute AI job postings, while D.C. has the highest proportion. Nationally, postings mentioning AI skills rose 20% year-over-year in 2024, with generative AI skills nearly quadrupling (Lightcast and Stanford HAI (2025), 1–17). In contrast, non-AI job growth is broad-based: 33 states reported employment gains in 2024, led by Rochester, MN (+6.5%) and Stockton–Lodi, CA (+5.3%), driven by healthcare and logistics (U.S. Bureau of Labor Statistics (2023), 1–2).\nRemote work has stabilized at a “new normal.” Although job postings labeled remote declined from &gt;10% in 2022 to ~7.5% in May 2025 (Indeed Hiring Lab (2024)), telework rates remain far above pre-pandemic levels, with ~20% of workers telecommuting in 2024 (U.S. Bureau of Labor Statistics (2024), 12). By industry, remote adoption is highest in IT, finance, and management (Makridis (2024), 16; Bick, Blandin, and Mertens (2024), 7)), while healthcare, retail, and manufacturing remain predominantly on-site. Employers are shifting toward hybrid models, often requiring more in-office presence, though remote-capable roles continue to play a significant role (Barrero, Bloom, and Davis (2023), 20–21).\nTraditional tech hubs like Silicon Valley and New York City remain dominant in high-skill hiring, with over 65% of AI engineers located in these regions as of 2024. Despite rising costs and the expansion of remote work, their strong innovation ecosystems and dense talent networks preserve their competitive edge. At the same time, geographic diversification is accelerating: Miami and San Diego are emerging as fast-growing hubs, driven by lifestyle appeal, lower living costs, and, in Miami’s case, favorable tax policies. Miami saw a 12% increase in AI roles, while San Diego experienced a 7% rise in Big Tech hiring and raised $5.7 billion in venture capital. Conversely, former growth leaders like Austin and Houston are losing momentum, with startup employment declining by 6% and 10.9%, respectively, due to infrastructure gaps, cultural mismatch, and a shift back toward hybrid work models (SignalFire (2025)).\nGenerative AI introduces a reversal in the geography of job market exposure compared to earlier automation technologies. Whereas prior waves of automation—such as robotics and enterprise software—primarily disrupted rural and small-town economies by replacing routine manual or physical labor, generative AI disproportionately affects urban, high-skill, white-collar labor markets. These urban centers, including San Jose, San Francisco, New York, and Washington D.C., are home to occupations rich in cognitive, nonroutine tasks such as coding, writing, and data analysis—tasks highly susceptible to AI augmentation or displacement(Muro, Methkupally, and Kinder (2025)).\nAs a result, AI exposure rates in urban counties often exceed 40%, while rural counties average closer to 30%, reflecting both differing occupational structures and varying access to information-oriented industries. While rural areas may be more insulated from AI disruption, they are also less likely to benefit from AI-driven productivity gains. This spatial shift in exposure demands new policy attention focused on urban workforce reskilling, while also ensuring rural communities are not left behind in the emerging AI economy(Muro, Methkupally, and Kinder (2025))."
  },
  {
    "objectID": "index.html#what-do-you-expect-to-find-in-your-analysis",
    "href": "index.html#what-do-you-expect-to-find-in-your-analysis",
    "title": "Geographic and Remote Work Analysis: Job Market 2024",
    "section": "What do you expect to find in your analysis?",
    "text": "What do you expect to find in your analysis?\nOur research encompasses various aspects of geographic distribution and remote work trends. Specifically, we aim to investigate: Job growth patterns across different cities and states, including a comparative analysis of which metropolitan areas demonstrate the highest expansion rates for both AI-related and non-AI career opportunities. The evolution of remote work arrangements, examining whether remote positions are experiencing growth or decline and how this shift affects workforce distribution. Comprehensive analysis of tech hub dynamics, identifying whether traditional technology centers like Silicon Valley, Austin, and Boston continue to dominate the hiring landscape, or if emerging locations are gaining competitive advantages. The urban-rural divide in employment opportunities, investigating how job market conditions differ between metropolitan and rural areas for both AI and traditional professional roles, and what this means for workforce mobility and regional economic development."
  },
  {
    "objectID": "data_analysis.html#data-cleaning-preprocessing",
    "href": "data_analysis.html#data-cleaning-preprocessing",
    "title": "Data Analysis",
    "section": "1 Data Cleaning & Preprocessing",
    "text": "1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\n\n1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\nimport seaborn as sns\n\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\n\n\n1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = df.columns.str.strip().str.upper()\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"SALARY\" in df.columns:\n        df[\"SALARY\"] = pd.to_numeric(\n            df[\"SALARY\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\n\n\n1.3 Visualize & handle missing values\n\n# --- Missing values correlation heatmap (like your classmate) ---\n\ncols_with_na = [c for c in df.columns if df[c].isna().any()]\nsub = df[cols_with_na]\n\nif len(cols_with_na) &gt;= 2:\n    \n    if len(cols_with_na) &gt; 25:\n        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index\n        sub = sub[top_na_cols]\n\n    \n    na_corr = sub.isna().astype(int).corr()\n\n    \n    mask = np.triu(np.ones_like(na_corr, dtype=bool))\n\n    plt.figure(figsize=(11, 8))\n    sns.heatmap(\n        na_corr, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1,\n        annot=True, fmt=\".1f\", linewidths=.5,\n        cbar_kws={\"shrink\": .8}\n    )\n    plt.title(\"Missing Values Correlation Heatmap\", fontsize=14)\n    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n\n    FIG_MISS = \"figures/missing_values_corr_heatmap.png\"\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    \nelse:\n    log(\"No columns with missing values; skipping heatmap.\")\n\n\n\n\n\n\n\n\n\n\n1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\n\n\n1.5 Optional salary sanity filter\n\nif \"SALARY\" in df.columns:\n    bad = (df[\"SALARY\"] &lt; 1) | (df[\"SALARY\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"SALARY\"] = np.nan\n        med2 = df[\"SALARY\"].median()\n        df[\"SALARY\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")"
  },
  {
    "objectID": "data_analysis.html#exploratory-data-analysis-eda",
    "href": "data_analysis.html#exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "2 Exploratory Data Analysis (EDA)",
    "text": "2 Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n2.1 Job Postings by Industry\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n2.2 Salary Distribution by Industry\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Select top 10 industries by posting count\ntop10 = df[industry_col].value_counts().head(10).index\ndf_top10 = df[df[industry_col].isin(top10)]\n\n# --- Boxplot Visualization ---\npalette = sns.color_palette(\"Spectral\", n_colors=10)\n\nplt.figure(figsize=(12, 8))\n\nsns.boxplot(\n    data=df_top10,\n    y=industry_col,\n    x=salary_col,\n    palette=palette,\n    showfliers=True,     # show outlier\n    linewidth=1.2\n)\n\n# Add title & labels\nplt.title(\"Salary Distribution by Industry (Top 10 Industries)\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"Salary (USD)\", fontsize=14)\nplt.ylabel(\"Industry\", fontsize=14)\n\n# Light grid background like example\nsns.set_style(\"whitegrid\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n\nplt.tight_layout()\n\n# Save figure\nplt.savefig(\"figures/salary_distribution_by_industry_top10.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n/tmp/ipykernel_5773/3831417689.py:25: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nUse a box chart to show the salary distribution of each major recruitment industry. Unlike bar charts, bar charts only show averages, while box charts can reveal the median, fluctuation range and abnormal values in each industry. This helps job seekers not only understand which industries have higher average salaries, but also the extent of salary fluctuations and the areas with the greatest potential for salary growth.\nIn various industries, some knowledge-intensive and service-intensive fields (such as unclassified industries and customized computer programming services) have relatively high salary levels and are more dispersed. The median salary in these areas is relatively high, and the upper tail is long, which indicates that there are a large number of highly paid positions. Management consulting, computer system design, and direct health and medical insurance industries also show a large range of remuneration, reflecting the wide range of remuneration within these professions. In contrast, industries such as colleges and universities, certified public accountants, commercial banks and software publishers have a tighter scope and fewer extreme outliers, indicating that the remuneration range of these industries is more standardized and there is less room for improvement at the top of the salary distribution.\n\n\n2.3 Remote vs. On-Site Jobs\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce.\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# =========================================\n# 1️⃣ Clean REMOTE_TYPE_NAME\n# =========================================\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str)\n    .str.strip()\n    .str.lower()\n    .replace({\n        \"[none]\": None,\n        \"none\": None,\n        \"unknown\": None,\n        \"nan\": None,\n        \"na\": None,\n        \"null\": None,\n        \"\": None\n    })\n)\n\n# =========================================\n# 2️⃣ Re-group into Remote / Hybrid / Onsite\n# =========================================\ndef map_remote(x):\n    if pd.isna(x):\n        return \"Onsite\"\n    if \"remote\" in x and \"hybrid\" in x:\n        return \"Hybrid\"\n    if x == \"hybrid remote\":\n        return \"Hybrid\"\n    if x == \"remote\":\n        return \"Remote\"\n    if x == \"not remote\":\n        return \"Onsite\"\n    return \"Onsite\"\n\ndf[\"REMOTE_GROUP\"] = df[\"REMOTE_TYPE_NAME\"].apply(map_remote)\n\nprint(df[\"REMOTE_GROUP\"].value_counts())\n\n# =========================================\n# 3️⃣ Visualization (Pie Chart)\n# =========================================\nremote_counts = df[\"REMOTE_GROUP\"].value_counts()\n\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. Hybrid vs. Onsite Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nREMOTE_GROUP\nOnsite    55304\nRemote    11743\nHybrid     2151\nName: count, dtype: int64"
  },
  {
    "objectID": "final_report.html",
    "href": "final_report.html",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "",
    "text": "x"
  },
  {
    "objectID": "final_report.html#data-cleaning-preprocessing-1",
    "href": "final_report.html#data-cleaning-preprocessing-1",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "2.1 Data Cleaning & Preprocessing",
    "text": "2.1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\n\n2.1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\nimport seaborn as sns\n\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\n\n\n2.1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = df.columns.str.strip().str.upper()\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"SALARY\" in df.columns:\n        df[\"SALARY\"] = pd.to_numeric(\n            df[\"SALARY\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\n\n\n2.1.3 Visualize & handle missing values\n\n# --- Missing values correlation heatmap (like your classmate) ---\n\ncols_with_na = [c for c in df.columns if df[c].isna().any()]\nsub = df[cols_with_na]\n\nif len(cols_with_na) &gt;= 2:\n    \n    if len(cols_with_na) &gt; 25:\n        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index\n        sub = sub[top_na_cols]\n\n    \n    na_corr = sub.isna().astype(int).corr()\n\n    \n    mask = np.triu(np.ones_like(na_corr, dtype=bool))\n\n    plt.figure(figsize=(11, 8))\n    sns.heatmap(\n        na_corr, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1,\n        annot=True, fmt=\".1f\", linewidths=.5,\n        cbar_kws={\"shrink\": .8}\n    )\n    plt.title(\"Missing Values Correlation Heatmap\", fontsize=14)\n    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n\n    FIG_MISS = \"figures/missing_values_corr_heatmap.png\"\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    \nelse:\n    log(\"No columns with missing values; skipping heatmap.\")\n\n\n\n\n\n\n\n\n\n\n2.1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\n\n\n2.1.5 Optional salary sanity filter\n\nif \"SALARY\" in df.columns:\n    bad = (df[\"SALARY\"] &lt; 1) | (df[\"SALARY\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"SALARY\"] = np.nan\n        med2 = df[\"SALARY\"].median()\n        df[\"SALARY\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n2.1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")"
  },
  {
    "objectID": "final_report.html#exploratory-data-analysis-eda",
    "href": "final_report.html#exploratory-data-analysis-eda",
    "title": "Machine Learning Models for Geographic and Remote Work Analysis",
    "section": "2.2 Exploratory Data Analysis (EDA)",
    "text": "2.2 Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.\nIn this section, we focus on three aspects: 1. Job postings by industry\n2. Salary distributions\n3. Remote vs. on-site job proportions\n\n\n2.2.1 Job Postings by Industry\nUnderstanding industry demand helps reveal which sectors are most active in hiring.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# Load dataset\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Ensure /figures directory exists\nos.makedirs(\"figures\", exist_ok=True)\n\n# col\nindustry_col = \"NAICS_2022_6_NAME\"\n\n# Clean up column names\ndf.columns = df.columns.str.strip()\n\n#drop unclassified\ndf = df[~df[\"NAICS_2022_6_NAME\"].str.lower().str.contains(\"unclassified\", na=False)]\n\n# Count top 10 industries\ntop_industries = df[industry_col].value_counts().head(10)\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_industries.values, y=top_industries.index, orient=\"h\")\nplt.title(\"Top 10 Industries by Job Postings\")\nplt.xlabel(\"Number of Job Postings\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.savefig(\"figures/industry_postings.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.\nJob demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.\n\n\n2.2.2 Salary Distribution by Industry\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Define column names\nindustry_col = \"NAICS_2022_6_NAME\"\nsalary_col = \"SALARY\"\n\n# Filter and clean salary\ndf = df[df[salary_col] &gt; 0]\n#df = df[~df[industry_col].str.lower().str.contains(\"unclassified\")]\n\n# Select top 10 industries by posting count\ntop10 = df[industry_col].value_counts().head(10).index\ndf_top10 = df[df[industry_col].isin(top10)]\n\n# --- Boxplot Visualization ---\npalette = sns.color_palette(\"Spectral\", n_colors=10)\n\nplt.figure(figsize=(12, 8))\n\nsns.boxplot(\n    data=df_top10,\n    y=industry_col,\n    x=salary_col,\n    palette=palette,\n    showfliers=True,     # show outlier\n    linewidth=1.2\n)\n\n# Add title & labels\nplt.title(\"Salary Distribution by Industry (Top 10 Industries)\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"Salary (USD)\", fontsize=14)\nplt.ylabel(\"Industry\", fontsize=14)\n\n# Light grid background like example\nsns.set_style(\"whitegrid\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n\nplt.tight_layout()\n\n# Save figure\nplt.savefig(\"figures/salary_distribution_by_industry_top10.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n/tmp/ipykernel_5950/3831417689.py:25: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nUse a box chart to show the salary distribution of each major recruitment industry. Unlike bar charts, bar charts only show averages, while box charts can reveal the median, fluctuation range and abnormal values in each industry. This helps job seekers not only understand which industries have higher average salaries, but also the extent of salary fluctuations and the areas with the greatest potential for salary growth.\nIn various industries, some knowledge-intensive and service-intensive fields (such as unclassified industries and customized computer programming services) have relatively high salary levels and are more dispersed. The median salary in these areas is relatively high, and the upper tail is long, which indicates that there are a large number of highly paid positions. Management consulting, computer system design, and direct health and medical insurance industries also show a large range of remuneration, reflecting the wide range of remuneration within these professions. In contrast, industries such as colleges and universities, certified public accountants, commercial banks and software publishers have a tighter scope and fewer extreme outliers, indicating that the remuneration range of these industries is more standardized and there is less room for improvement at the top of the salary distribution.\n\n\n2.2.3 Remote vs. On-Site Jobs\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# Clean the REMOTE_TYPE_NAME column\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str).str.strip().str.lower()\n    .replace({\"[none]\": None, \"none\": None, \"unknown\": None, \"nan\": None, \"na\": None, \"null\": None, \"\": None})\n)\n\n# Count each type\nremote_counts = df[\"REMOTE_TYPE_NAME\"].value_counts()\n\n# Visual\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. On-Site Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nA pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.\nThe job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce.\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.makedirs(\"figures\", exist_ok=True)\n\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\", low_memory=False)\n\n# =========================================\n# 1️⃣ Clean REMOTE_TYPE_NAME\n# =========================================\ndf[\"REMOTE_TYPE_NAME\"] = (\n    df[\"REMOTE_TYPE_NAME\"]\n    .astype(str)\n    .str.strip()\n    .str.lower()\n    .replace({\n        \"[none]\": None,\n        \"none\": None,\n        \"unknown\": None,\n        \"nan\": None,\n        \"na\": None,\n        \"null\": None,\n        \"\": None\n    })\n)\n\n# =========================================\n# 2️⃣ Re-group into Remote / Hybrid / Onsite\n# =========================================\ndef map_remote(x):\n    if pd.isna(x):\n        return \"Onsite\"\n    if \"remote\" in x and \"hybrid\" in x:\n        return \"Hybrid\"\n    if x == \"hybrid remote\":\n        return \"Hybrid\"\n    if x == \"remote\":\n        return \"Remote\"\n    if x == \"not remote\":\n        return \"Onsite\"\n    return \"Onsite\"\n\ndf[\"REMOTE_GROUP\"] = df[\"REMOTE_TYPE_NAME\"].apply(map_remote)\n\nprint(df[\"REMOTE_GROUP\"].value_counts())\n\n# =========================================\n# 3️⃣ Visualization (Pie Chart)\n# =========================================\nremote_counts = df[\"REMOTE_GROUP\"].value_counts()\n\nplt.figure(figsize=(6, 6))\nplt.pie(\n    remote_counts.values,\n    labels=remote_counts.index,\n    autopct=\"%1.1f%%\",\n    startangle=90,\n    wedgeprops={\"edgecolor\": \"white\"}\n)\nplt.title(\"Remote vs. Hybrid vs. Onsite Job Distribution\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"figures/remote_vs_onsite.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nREMOTE_GROUP\nOnsite    55304\nRemote    11743\nHybrid     2151\nName: count, dtype: int64"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Analysis",
    "section": "",
    "text": "1 Create a team-based skill dataframe\n\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Bingrui Qiao\", \"Zhengyu Zhou\", \"Junhao Wang\"],\n    \"Python\": [2, 2, 3],\n    \"SQL\": [2, 2, 3],\n    \"Machine Learning\": [1, 1, 1],\n    \"Cloud Computing\": [2, 2, 3],\n    \"Docker\": [1, 1, 1],\n    \"AWS\": [2, 3, 2]\n}\n\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\nDocker\nAWS\n\n\nName\n\n\n\n\n\n\n\n\n\n\nBingrui Qiao\n2\n2\n1\n2\n1\n2\n\n\nZhengyu Zhou\n2\n2\n1\n2\n1\n3\n\n\nJunhao Wang\n3\n3\n1\n3\n1\n2\n\n\n\n\n\n\n\n\n\n2 Visualizing Skill Gaps\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#visual\nsns.set_theme(style=\"whitegrid\", font_scale=1.1, rc={\n    \"axes.facecolor\": \"white\",\n    \"figure.facecolor\": \"white\",\n    \"axes.edgecolor\": \"gray\",\n    \"grid.color\": \"lightgray\"\n})\n\n# Color choice\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# visual\nplt.figure(figsize=(9, 6))\nax = sns.heatmap(\n    df_skills,\n    annot=True,\n    fmt=\".0f\",\n    cmap=cmap,\n    linewidths=0.7,\n    cbar_kws={'label': 'Skill Level'},\n    square=True\n)\n\n# Title\nplt.title(\"Team Skill Levels Heatmap\", fontsize=15, weight=\"bold\", pad=20)\nplt.xlabel(\"Skill Domain\", fontsize=12)\nplt.ylabel(\"Team Member\", fontsize=12)\n\n# layout\nplt.xticks(rotation=30, ha=\"right\")\nplt.yticks(rotation=0)\nplt.tight_layout()\n\n#save\nplt.savefig(\"figures/Skill_Gap_Analysis.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3 Compare skill\n\n# Load data\ndf = pd.read_csv(\"data/cleaned_lightcast.csv\")\n\n# Count keyword occurrences\ntop_skills = df_skills.columns.tolist()\njob_text = df[\"BODY\"].fillna(\"\")\nskill_counts = {s: job_text.str.contains(s, case=False).sum() for s in top_skills}\n\n# Append demand row\ndf_skills.loc[\"Market Demand\"] = [skill_counts[s] for s in top_skills]\ndf_skills\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\nDocker\nAWS\n\n\nName\n\n\n\n\n\n\n\n\n\n\nBingrui Qiao\n2\n2\n1\n2\n1\n2\n\n\nZhengyu Zhou\n2\n2\n1\n2\n1\n3\n\n\nJunhao Wang\n3\n3\n1\n3\n1\n2\n\n\nMarket Demand\n11782\n23202\n3972\n1302\n688\n14243\n\n\n\n\n\n\n\n\n\n4 Visual heatmap\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Same style\nsns.set_theme(style=\"whitegrid\", font_scale=1.1, rc={\n    \"axes.facecolor\": \"white\",\n    \"figure.facecolor\": \"white\",\n    \"axes.edgecolor\": \"gray\",\n    \"grid.color\": \"lightgray\"\n})\n\n\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# Team Skill Levels\nplt.figure(figsize=(8, 2.5))\nplt.imshow(df_skills.iloc[:-1], aspect=\"auto\", cmap=cmap)\nplt.colorbar(label=\"Skill Level (1–5)\")\nplt.yticks(range(len(df_skills.index)-1), df_skills.index[:-1], fontsize=10)\nplt.xticks(range(len(df_skills.columns)), df_skills.columns, rotation=30, ha=\"right\", fontsize=9)\nplt.title(\"Team Skill Levels\", fontsize=13, weight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"figures/Team_Skill_Level.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# Market Demand Heatmap\nplt.figure(figsize=(8, 2))\nplt.imshow([df_skills.loc[\"Market Demand\"]], aspect=\"auto\", cmap=cmap)\nplt.colorbar(label=\"Market Demand Count\")\nplt.yticks([0], [\"Market Demand\"], fontsize=10)\nplt.xticks(range(len(df_skills.columns)), df_skills.columns, rotation=30, ha=\"right\", fontsize=9)\nplt.title(\"Market Demand\", fontsize=13, weight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"figures/Market_Demand.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "data_cleaning.html#data-cleaning-preprocessing",
    "href": "data_cleaning.html#data-cleaning-preprocessing",
    "title": "Data Cleaning",
    "section": "1 Data Cleaning & Preprocessing",
    "text": "1 Data Cleaning & Preprocessing\nIn this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.\n\nimport os, datetime\nos.makedirs(\"logs\", exist_ok=True)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello @ \" + str(datetime.datetime.now()) + \"\\n\")\nprint(\"WROTE: logs/_ping.txt\")\n\n\n1.1 Setup & Load (clean version)\n\nimport os, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport subprocess\nimport seaborn as sns\n\n\n# Handle missingno (optional)\ntry:\n    import missingno as msno\n    HAS_MSNO = True\nexcept ImportError:\n    HAS_MSNO = False\n\n# Paths\nDATA_PATH  = \"data/lightcast_job_postings.csv\"\nCLEAN_PATH = \"data/cleaned_lightcast.csv\"\nLOG_PATH   = \"logs/cleaning_log.txt\"\nFIG_MISS   = \"figures/missing_values_heatmap.png\"\n\n# Ensure output dirs exist\nos.makedirs(\"logs\", exist_ok=True)\nos.makedirs(\"figures\", exist_ok=True)\n\n# Logger\ndef log(msg: str):\n    print(msg)\n    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n        f.write(msg.rstrip() + \"\\n\")\n\n# Start a fresh log\nwith open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"=== DATA CLEANING LOG START ===\\n\")\n\n# Ping file (to confirm write permission)\nwith open(\"logs/_ping.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"hello from python @ \" + str(datetime.datetime.now()) + \"\\n\")\n\n# AUTO-DOWNLOAD IF MISSING\nif not os.path.exists(DATA_PATH):\n    gdrive_url = \"https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ\"\n    try:\n        import gdown\n        gdown.download(gdrive_url, DATA_PATH, quiet=False)\n    except Exception as e:\n        raise FileNotFoundError(f\"Could not download dataset.\\nError: {e}\")\n\n\n# Load data\nif not os.path.exists(DATA_PATH):\n    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Check path & working dir.\")\ndf = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines=\"skip\")\nlog(f\"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}\")\n\n\n\n1.2 Drop redundant/irrelevant columns\n\ncolumns_to_drop = [\n\"ID\",\"URL\",\"ACTIVE_URLS\",\"DUPLICATES\",\"LAST_UPDATED_TIMESTAMP\",\n\"NAICS2\",\"NAICS3\",\"NAICS4\",\"NAICS5\",\"NAICS6\",\n\"SOC_2\",\"SOC_3\",\"SOC_5\"\n]\nbefore_cols = df.shape[1]\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\nafter_cols = df.shape[1]\nlog(f\"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}\")\n\n# Normalize names & basic types\n\ndf.columns = df.columns.str.strip().str.upper()\n\nif \"POSTED\" in df.columns:\n        df[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\n\nif \"SALARY\" in df.columns:\n        df[\"SALARY\"] = pd.to_numeric(\n            df[\"SALARY\"].astype(str).str.replace(r\"[^0-9.-]\", \"\", regex=True),\n                errors=\"coerce\"\n)\n\n\n\n1.3 Visualize & handle missing values\n\n# --- Missing values correlation heatmap (like your classmate) ---\n\ncols_with_na = [c for c in df.columns if df[c].isna().any()]\nsub = df[cols_with_na]\n\nif len(cols_with_na) &gt;= 2:\n    \n    if len(cols_with_na) &gt; 25:\n        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index\n        sub = sub[top_na_cols]\n\n    \n    na_corr = sub.isna().astype(int).corr()\n\n    \n    mask = np.triu(np.ones_like(na_corr, dtype=bool))\n\n    plt.figure(figsize=(11, 8))\n    sns.heatmap(\n        na_corr, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1,\n        annot=True, fmt=\".1f\", linewidths=.5,\n        cbar_kws={\"shrink\": .8}\n    )\n    plt.title(\"Missing Values Correlation Heatmap\", fontsize=14)\n    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n\n    FIG_MISS = \"figures/missing_values_corr_heatmap.png\"\n    plt.savefig(FIG_MISS, dpi=150)\n    plt.show()\n    \nelse:\n    log(\"No columns with missing values; skipping heatmap.\")\n\n\n\n\n\n\n\n\n\n\n1.4 Remove duplicates\n\nsubset_cols = [c for c in [\"TITLE\",\"COMPANY\",\"LOCATION\",\"POSTED\"] if c in df.columns]\nbefore = len(df)\nif subset_cols:\n    df.drop_duplicates(subset=subset_cols, keep=\"first\", inplace=True)\n    after = len(df)\n    log(f\"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}\")\n\n\n\n1.5 Optional salary sanity filter\n\nif \"SALARY\" in df.columns:\n    bad = (df[\"SALARY\"] &lt; 1) | (df[\"SALARY\"] &gt; 1_000_000)\n    n_bad = int(bad.sum())\n    if n_bad &gt; 0:\n        df.loc[bad, \"SALARY\"] = np.nan\n        med2 = df[\"SALARY\"].median()\n        df[\"SALARY\"].fillna(med2, inplace=True)\n        log(f\"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}\")\n\n\n\n1.6 Save & summary\n\ndf.to_csv(CLEAN_PATH, index=False)\nlog(f\"Saved cleaned dataset → {CLEAN_PATH}\")\n\nsummary = f\"Rows: {len(df):,}\\nColumns: {df.shape[1]}\\nSample columns: {list(df.columns)[:12]}\"\nprint(summary)\nlog(\"✅ Cleaning pipeline finished successfully.\")"
  }
]