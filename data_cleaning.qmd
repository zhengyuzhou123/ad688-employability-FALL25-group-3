---
title: "Data Cleaning"
subtitle: "Comprehensive Data Cleaning "
author:
  - name: "Bingrui Qiao"
  - name: "Zhengyu Zhou"
  - name: "Junhao Wang"
    affiliations: "Boston University"
bibliography: references.bib
csl: csl/econometrica.csl
format:
  html:
    toc: true
    number-sections: true
execute:
  echo: true
  eval: true
  freeze: false
  error: false
  cache: false
  enabled: !expr (os.getenv("CI", "false") != "true")
jupyter: python3
---



---

## Data Cleaning & Preprocessing

In this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.
```{python}
#| echo: false
#| output: false
# --- 0.0 Ensure dependencies are installed (for CI / Quarto Cloud) ---
import sys, subprocess

# package
for pkg in ["gdown", "pandas", "matplotlib", "missingno", "pyarrow"]:
    subprocess.run([sys.executable, "-m", "pip", "install", pkg], check=False)
```
```{python}
#| output: false
import os, datetime
os.makedirs("logs", exist_ok=True)
with open("logs/_ping.txt", "w", encoding="utf-8") as f:
    f.write("hello @ " + str(datetime.datetime.now()) + "\n")
print("WROTE: logs/_ping.txt")
```

### Setup & Load (clean version)
```{python}
#| output: false
import os, datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import missingno as msno
import subprocess
import seaborn as sns


# Handle missingno (optional)
try:
    import missingno as msno
    HAS_MSNO = True
except ImportError:
    HAS_MSNO = False

# Paths
DATA_PATH  = "data/lightcast_job_postings.csv"
CLEAN_PATH = "data/cleaned_lightcast.csv"
LOG_PATH   = "logs/cleaning_log.txt"
FIG_MISS   = "figures/missing_values_heatmap.png"

# Ensure output dirs exist
os.makedirs("logs", exist_ok=True)
os.makedirs("figures", exist_ok=True)

# Logger
def log(msg: str):
    print(msg)
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(msg.rstrip() + "\n")

# Start a fresh log
with open(LOG_PATH, "w", encoding="utf-8") as f:
    f.write("=== DATA CLEANING LOG START ===\n")

# Ping file (to confirm write permission)
with open("logs/_ping.txt", "w", encoding="utf-8") as f:
    f.write("hello from python @ " + str(datetime.datetime.now()) + "\n")

# AUTO-DOWNLOAD IF MISSING
if not os.path.exists(DATA_PATH):
    gdrive_url = "https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ"
    try:
        import gdown
        gdown.download(gdrive_url, DATA_PATH, quiet=False)
    except Exception as e:
        raise FileNotFoundError(f"Could not download dataset.\nError: {e}")


# Load data
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"Dataset not found at {DATA_PATH}. Check path & working dir.")
df = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines="skip")
log(f"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}")
```

### Drop redundant/irrelevant columns
```{python}
#| output: false
columns_to_drop = [
"ID","URL","ACTIVE_URLS","DUPLICATES","LAST_UPDATED_TIMESTAMP",
"NAICS2","NAICS3","NAICS4","NAICS5","NAICS6",
"SOC_2","SOC_3","SOC_5"
]
before_cols = df.shape[1]
df.drop(columns=columns_to_drop, inplace=True, errors="ignore")
after_cols = df.shape[1]
log(f"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}")

# Normalize names & basic types

df.columns = df.columns.str.strip().str.upper()

if "POSTED" in df.columns:
        df["POSTED"] = pd.to_datetime(df["POSTED"], errors="coerce")

if "SALARY" in df.columns:
        df["SALARY"] = pd.to_numeric(
            df["SALARY"].astype(str).str.replace(r"[^0-9.-]", "", regex=True),
                errors="coerce"
)
```

### Visualize & handle missing values
```{python}

# --- Missing values correlation heatmap (like your classmate) ---

cols_with_na = [c for c in df.columns if df[c].isna().any()]
sub = df[cols_with_na]

if len(cols_with_na) >= 2:
    
    if len(cols_with_na) > 25:
        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index
        sub = sub[top_na_cols]

    
    na_corr = sub.isna().astype(int).corr()

    
    mask = np.triu(np.ones_like(na_corr, dtype=bool))

    plt.figure(figsize=(11, 8))
    sns.heatmap(
        na_corr, mask=mask, cmap="coolwarm", vmin=-1, vmax=1,
        annot=True, fmt=".1f", linewidths=.5,
        cbar_kws={"shrink": .8}
    )
    plt.title("Missing Values Correlation Heatmap", fontsize=14)
    plt.xticks(rotation=45, ha="right", fontsize=8)
    plt.yticks(rotation=0, fontsize=8)
    plt.tight_layout()

    FIG_MISS = "figures/missing_values_corr_heatmap.png"
    plt.savefig(FIG_MISS, dpi=150)
    plt.show()
    
else:
    log("No columns with missing values; skipping heatmap.")

```

### Remove duplicates
```{python}
#| output: false
subset_cols = [c for c in ["TITLE","COMPANY","LOCATION","POSTED"] if c in df.columns]
before = len(df)
if subset_cols:
    df.drop_duplicates(subset=subset_cols, keep="first", inplace=True)
    after = len(df)
    log(f"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}")
```

### Optional salary sanity filter
```{python}

if "SALARY" in df.columns:
    bad = (df["SALARY"] < 1) | (df["SALARY"] > 1_000_000)
    n_bad = int(bad.sum())
    if n_bad > 0:
        df.loc[bad, "SALARY"] = np.nan
        med2 = df["SALARY"].median()
        df["SALARY"].fillna(med2, inplace=True)
        log(f"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}")
```

### Save & summary
```{python}
#| output: false
df.to_csv(CLEAN_PATH, index=False)
log(f"Saved cleaned dataset → {CLEAN_PATH}")

summary = f"Rows: {len(df):,}\nColumns: {df.shape[1]}\nSample columns: {list(df.columns)[:12]}"
print(summary)
log("✅ Cleaning pipeline finished successfully.")
```