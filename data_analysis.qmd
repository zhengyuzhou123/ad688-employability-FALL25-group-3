
---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis"
author:
  - name: "Bingrui Qiao"
  - name: "Zhengyu Zhou"
  - name: "Junhao Wang"
    affiliations: "Boston University"
bibliography: references.bib
csl: csl/econometrica.csl
format:
  html:
    toc: true
    number-sections: true
execute:
  echo: true
  eval: true
  freeze: false
  error: false
  cache: false
  enabled: !expr (os.getenv("CI", "false") != "true")
jupyter: python3
---



---

## Data Cleaning & Preprocessing

In this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.
```{python}
#| echo: false
#| output: false
# --- 0.0 Ensure dependencies are installed (for CI / Quarto Cloud) ---
import sys, subprocess

# package
for pkg in ["gdown", "pandas", "matplotlib", "missingno", "pyarrow"]:
    subprocess.run([sys.executable, "-m", "pip", "install", pkg], check=False)
```
```{python}
#| output: false
import os, datetime
os.makedirs("logs", exist_ok=True)
with open("logs/_ping.txt", "w", encoding="utf-8") as f:
    f.write("hello @ " + str(datetime.datetime.now()) + "\n")
print("WROTE: logs/_ping.txt")
```

### Setup & Load (clean version)
```{python}
#| output: false
import os, datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import missingno as msno
import subprocess
import seaborn as sns


# Handle missingno (optional)
try:
    import missingno as msno
    HAS_MSNO = True
except ImportError:
    HAS_MSNO = False

# Paths
DATA_PATH  = "data/lightcast_job_postings.csv"
CLEAN_PATH = "data/cleaned_lightcast.csv"
LOG_PATH   = "logs/cleaning_log.txt"
FIG_MISS   = "figures/missing_values_heatmap.png"

# Ensure output dirs exist
os.makedirs("logs", exist_ok=True)
os.makedirs("figures", exist_ok=True)

# Logger
def log(msg: str):
    print(msg)
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(msg.rstrip() + "\n")

# Start a fresh log
with open(LOG_PATH, "w", encoding="utf-8") as f:
    f.write("=== DATA CLEANING LOG START ===\n")

# Ping file (to confirm write permission)
with open("logs/_ping.txt", "w", encoding="utf-8") as f:
    f.write("hello from python @ " + str(datetime.datetime.now()) + "\n")

# AUTO-DOWNLOAD IF MISSING
if not os.path.exists(DATA_PATH):
    gdrive_url = "https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ"
    try:
        import gdown
        gdown.download(gdrive_url, DATA_PATH, quiet=False)
    except Exception as e:
        raise FileNotFoundError(f"Could not download dataset.\nError: {e}")


# Load data
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"Dataset not found at {DATA_PATH}. Check path & working dir.")
df = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines="skip")
log(f"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}")
```

### Drop redundant/irrelevant columns
```{python}
#| output: false
columns_to_drop = [
"ID","URL","ACTIVE_URLS","DUPLICATES","LAST_UPDATED_TIMESTAMP",
"NAICS2","NAICS3","NAICS4","NAICS5","NAICS6",
"SOC_2","SOC_3","SOC_5"
]
before_cols = df.shape[1]
df.drop(columns=columns_to_drop, inplace=True, errors="ignore")
after_cols = df.shape[1]
log(f"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}")

# Normalize names & basic types

df.columns = df.columns.str.strip().str.upper()

if "POSTED" in df.columns:
        df["POSTED"] = pd.to_datetime(df["POSTED"], errors="coerce")

if "SALARY" in df.columns:
        df["SALARY"] = pd.to_numeric(
            df["SALARY"].astype(str).str.replace(r"[^0-9.-]", "", regex=True),
                errors="coerce"
)
```

### Visualize & handle missing values
```{python}

# --- Missing values correlation heatmap (like your classmate) ---

cols_with_na = [c for c in df.columns if df[c].isna().any()]
sub = df[cols_with_na]

if len(cols_with_na) >= 2:
    
    if len(cols_with_na) > 25:
        top_na_cols = sub.isna().mean().sort_values(ascending=False).head(25).index
        sub = sub[top_na_cols]

    
    na_corr = sub.isna().astype(int).corr()

    
    mask = np.triu(np.ones_like(na_corr, dtype=bool))

    plt.figure(figsize=(11, 8))
    sns.heatmap(
        na_corr, mask=mask, cmap="coolwarm", vmin=-1, vmax=1,
        annot=True, fmt=".1f", linewidths=.5,
        cbar_kws={"shrink": .8}
    )
    plt.title("Missing Values Correlation Heatmap", fontsize=14)
    plt.xticks(rotation=45, ha="right", fontsize=8)
    plt.yticks(rotation=0, fontsize=8)
    plt.tight_layout()

    FIG_MISS = "figures/missing_values_corr_heatmap.png"
    plt.savefig(FIG_MISS, dpi=150)
    plt.show()
    
else:
    log("No columns with missing values; skipping heatmap.")

```

### Remove duplicates
```{python}
#| output: false
subset_cols = [c for c in ["TITLE","COMPANY","LOCATION","POSTED"] if c in df.columns]
before = len(df)
if subset_cols:
    df.drop_duplicates(subset=subset_cols, keep="first", inplace=True)
    after = len(df)
    log(f"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}")
```

### Optional salary sanity filter
```{python}

if "SALARY" in df.columns:
    bad = (df["SALARY"] < 1) | (df["SALARY"] > 1_000_000)
    n_bad = int(bad.sum())
    if n_bad > 0:
        df.loc[bad, "SALARY"] = np.nan
        med2 = df["SALARY"].median()
        df["SALARY"].fillna(med2, inplace=True)
        log(f"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}")
```

### Save & summary
```{python}
#| output: false
df.to_csv(CLEAN_PATH, index=False)
log(f"Saved cleaned dataset → {CLEAN_PATH}")

summary = f"Rows: {len(df):,}\nColumns: {df.shape[1]}\nSample columns: {list(df.columns)[:12]}"
print(summary)
log("✅ Cleaning pipeline finished successfully.")
```

## Exploratory Data Analysis (EDA)

Exploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.  
In this section, we focus on three aspects:
1. Job postings by industry  
2. Salary distributions  
3. Remote vs. on-site job proportions

---

### Job Postings by Industry

Understanding industry demand helps reveal which sectors are most active in hiring.

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Load dataset
df = pd.read_csv("data/cleaned_lightcast.csv", low_memory=False)

# Ensure /figures directory exists
os.makedirs("figures", exist_ok=True)

# col
industry_col = "NAICS_2022_6_NAME"

# Clean up column names
df.columns = df.columns.str.strip()

#drop unclassified
df = df[~df["NAICS_2022_6_NAME"].str.lower().str.contains("unclassified", na=False)]

# Count top 10 industries
top_industries = df[industry_col].value_counts().head(10)


plt.figure(figsize=(10, 6))
sns.barplot(x=top_industries.values, y=top_industries.index, orient="h")
plt.title("Top 10 Industries by Job Postings")
plt.xlabel("Number of Job Postings")
plt.ylabel("Industry")
plt.tight_layout()
plt.savefig("figures/industry_postings.png", dpi=300)
plt.show()
```

A bar chart was chosen to visualize the number of job postings across different industries. This format makes it easy to compare industry demand and helps job seekers understand which sectors have the highest hiring activity.

Job demand is concentrated in tech and professional services: Custom Computer Programming Services leads, followed closely by Management Consulting, Employment Placement Agencies, and Computer Systems Design—each with ~4–5k postings. After these, volumes drop sharply to a second tier (Commercial Banking, CPA offices, Temporary Help, Health/Medical Insurance, Other Computer Services, Colleges/Universities) at ~1.3–2.0k. The prominence of staffing/placement agencies signals broad, economy-wide hiring, while the tech-heavy top ranks highlight sustained demand for digital and knowledge-worker skills.


### Salary Distribution by Industry
```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

os.makedirs("figures", exist_ok=True)

df = pd.read_csv("data/cleaned_lightcast.csv", low_memory=False)

# Define column names
industry_col = "NAICS_2022_6_NAME"
salary_col = "SALARY"

# Filter and clean salary
df = df[df[salary_col] > 0]
#df = df[~df[industry_col].str.lower().str.contains("unclassified")]

# Select top 10 industries by posting count
top10 = df[industry_col].value_counts().head(10).index
df_top10 = df[df[industry_col].isin(top10)]

# --- Boxplot Visualization ---
palette = sns.color_palette("Spectral", n_colors=10)

plt.figure(figsize=(12, 8))

sns.boxplot(
    data=df_top10,
    y=industry_col,
    x=salary_col,
    palette=palette,
    showfliers=True,     # show outliers
    linewidth=1.2
)

# Add title & labels
plt.title("Salary Distribution by Industry (Top 10 Industries)", fontsize=18, weight="bold")
plt.xlabel("Salary (USD)", fontsize=14)
plt.ylabel("Industry", fontsize=14)

# Light grid background like example
sns.set_style("whitegrid")
plt.grid(axis="x", linestyle="--", alpha=0.3)

plt.tight_layout()

# Save figure
plt.savefig("figures/salary_distribution_by_industry_top10.png", dpi=300, bbox_inches="tight")
plt.show()
```

Use a box charts to show the salary distribution of each major recruitment industry. Unlike bar charts, bar charts only show averages, while box charts can reveal the median, fluctuation range and abnormal values in each industry. This helps job seekers not only understand which industries have higher average salaries, but also the extent of salary fluctuations and the areas with the greatest potential for salary growth.

In various industries, some knowledge-intensive and service-intensive fields (such as unclassified industries and customized computer programming services) have relatively high salary levels and are more dispersed. The median salary in these areas is relatively high, and the upper tail is long, which indicates that there are a large number of highly paid positions. Management consulting, computer system design, and direct health and medical insurance industries also show a large range of remuneration, reflecting the wide range of remuneration within these professions. In contrast, industries such as colleges and universities, certified public accountants, commercial banks and software publishers have a tighter scope and fewer extreme outliers, indicating that the remuneration range of these industries is more standardized and there is less room for improvement at the top of the salary distribution.

### Remote vs. On-Site Jobs
```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

os.makedirs("figures", exist_ok=True)

df = pd.read_csv("data/cleaned_lightcast.csv", low_memory=False)

# Clean the REMOTE_TYPE_NAME column
df["REMOTE_TYPE_NAME"] = (
    df["REMOTE_TYPE_NAME"]
    .astype(str).str.strip().str.lower()
    .replace({"[none]": None, "none": None, "unknown": None, "nan": None, "na": None, "null": None, "": None})
)

# Count each type
remote_counts = df["REMOTE_TYPE_NAME"].value_counts()

# Visual
plt.figure(figsize=(6, 6))
plt.pie(
    remote_counts.values,
    labels=remote_counts.index,
    autopct="%1.1f%%",
    startangle=90,
    wedgeprops={"edgecolor": "white"}
)
plt.title("Remote vs. On-Site Job Distribution", fontsize=13)
plt.tight_layout()
plt.savefig("figures/remote_vs_onsite.png", dpi=300, bbox_inches="tight")
plt.show()
```

A pie chart was selected to display the proportions of job types (remote, hybrid, and on-site). This format offers an intuitive visual summary, helping job seekers understand the flexibility of job opportunities in 2024.

The job market here is overwhelmingly remote: nearly four out of five postings (78.4%) are fully remote, while another 14.4% offer hybrid options. Only 7.3% require full on-site work. This mix signals strong employer flexibility and broad access to roles regardless of location, with hybrid emerging as a meaningful—but secondary—model. For candidates, remote-first skills (self-management, async collaboration) are likely at a premium, while fully on-site opportunities are comparatively scarce.
