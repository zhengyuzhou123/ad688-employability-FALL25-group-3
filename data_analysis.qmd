
---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis"
author:
  - name: "Bingrui Qiao"
    affiliations: "Boston University"
bibliography: references.bib
csl: csl/econometrica.csl
format:
  html:
    toc: true
    number-sections: true
execute:
  echo: true
  eval: true
  freeze: false
  error: false
  cache: false
  enabled: !expr (os.getenv("CI", "false") != "true")
jupyter: python3
---



---

## Data Cleaning & Preprocessing

In this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.
```{python}
#| echo: false
#| output: false
# --- 0.0 Ensure dependencies are installed (for CI / Quarto Cloud) ---
import sys, subprocess

# package
for pkg in ["gdown", "pandas", "matplotlib", "missingno", "pyarrow"]:
    subprocess.run([sys.executable, "-m", "pip", "install", pkg], check=False)
```
```{python}
import os, datetime
os.makedirs("logs", exist_ok=True)
with open("logs/_ping.txt", "w", encoding="utf-8") as f:
    f.write("hello @ " + str(datetime.datetime.now()) + "\n")
print("WROTE: logs/_ping.txt")
```

### Setup & Load (clean version)
```{python}
import os, datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import missingno as msno
import subprocess

# Handle missingno (optional)
try:
    import missingno as msno
    HAS_MSNO = True
except ImportError:
    HAS_MSNO = False

# Paths
DATA_PATH  = "data/lightcast_job_postings.csv"
CLEAN_PATH = "data/cleaned_lightcast.csv"
LOG_PATH   = "logs/cleaning_log.txt"
FIG_MISS   = "figures/missing_values_heatmap.png"

# Ensure output dirs exist
os.makedirs("logs", exist_ok=True)
os.makedirs("figures", exist_ok=True)

# Logger
def log(msg: str):
    print(msg)
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(msg.rstrip() + "\n")

# Start a fresh log
with open(LOG_PATH, "w", encoding="utf-8") as f:
    f.write("=== DATA CLEANING LOG START ===\n")

# Ping file (to confirm write permission)
with open("logs/_ping.txt", "w", encoding="utf-8") as f:
    f.write("hello from python @ " + str(datetime.datetime.now()) + "\n")

# AUTO-DOWNLOAD IF MISSING
if not os.path.exists(DATA_PATH):
    log(f"⚠️ Dataset missing at {DATA_PATH}. Attempting to download from Google Drive...")
    gdrive_url = "https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ"
    try:
        import gdown
        gdown.download(gdrive_url, DATA_PATH, quiet=False)
        log("✅ Dataset downloaded successfully.")
    except Exception as e:
        raise FileNotFoundError(f"❌ Could not download dataset.\nError: {e}")


# Load data
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"❌ Dataset not found at {DATA_PATH}. Check path & working dir.")
df = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines="skip")
log(f"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}")
```

### Drop redundant/irrelevant columns
```{python}

columns_to_drop = [
"ID","URL","ACTIVE_URLS","DUPLICATES","LAST_UPDATED_TIMESTAMP",
"NAICS2","NAICS3","NAICS4","NAICS5","NAICS6",
"SOC_2","SOC_3","SOC_5"
]
before_cols = df.shape[1]
df.drop(columns=columns_to_drop, inplace=True, errors="ignore")
after_cols = df.shape[1]
log(f"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}")

# Normalize names & basic types

df.columns = [c.strip() for c in df.columns]

if "POSTED" in df.columns:
        df["POSTED"] = pd.to_datetime(df["POSTED"], errors="coerce")

if "Salary" in df.columns:
        df["Salary"] = pd.to_numeric(
            df["Salary"].astype(str).str.replace(r"[^0-9.-]", "", regex=True),
                errors="coerce"
)
```

### Visualize & handle missing values
```{python}

if HAS_MSNO:
    plt.figure(figsize=(10,6))
    msno.heatmap(df)
    plt.title("Missing Values Heatmap", fontsize=14)
    plt.tight_layout()
    plt.savefig(FIG_MISS, dpi=150)
    plt.show()
    log(f"Saved missingness heatmap → {FIG_MISS}")
else:
    log("missingno not installed; skipping heatmap.")

# Log top-20 missing columns

na_pct = df.isna().mean().sort_values(ascending=False) * 100
log("Top-20 columns by missing%:")
for col, pct in na_pct.head(20).items():
    log(f"  - {col}: {pct:.1f}%")

# Drop columns with >50% missing

thresh = len(df) * 0.5
before_cols = df.shape[1]
df.dropna(axis=1, thresh=thresh, inplace=True)
after_cols = df.shape[1]
log(f"Dropped {before_cols - after_cols} high-missing columns (>50%). Remaining: {after_cols}")

# Impute numeric

if "Salary" in df.columns:
    med_salary = df["Salary"].median()
    df["Salary"].fillna(med_salary, inplace=True)
    log(f"Filled Salary NAs with median: {med_salary:.2f}")

# Impute categorical

for cat in ["Industry","REMOTE_TYPE_NAME","LOCATION"]:
    if cat in df.columns:
        df[cat] = df[cat].fillna("Unknown").astype(str).str.strip()
        log(f"Filled {cat} NAs with 'Unknown'")
```

### Remove duplicates
```{python}

subset_cols = [c for c in ["TITLE","COMPANY","LOCATION","POSTED"] if c in df.columns]
before = len(df)
if subset_cols:
    df.drop_duplicates(subset=subset_cols, keep="first", inplace=True)
    after = len(df)
    log(f"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}")
```

### Optional salary sanity filter
```{python}

if "Salary" in df.columns:
    bad = (df["Salary"] < 1) | (df["Salary"] > 1_000_000)
    n_bad = int(bad.sum())
    if n_bad > 0:
        df.loc[bad, "Salary"] = np.nan
        med2 = df["Salary"].median()
        df["Salary"].fillna(med2, inplace=True)
        log(f"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}")
```

### Save & summary
```{python}

df.to_csv(CLEAN_PATH, index=False)
log(f"Saved cleaned dataset → {CLEAN_PATH}")

summary = f"Rows: {len(df):,}\nColumns: {df.shape[1]}\nSample columns: {list(df.columns)[:12]}"
print(summary)
log("✅ Cleaning pipeline finished successfully.")
```

## 5 Exploratory Data Analysis (EDA)

Exploratory Data Analysis (EDA) allows us to identify patterns and distributions in the job market dataset.  
In this section, we focus on three aspects:
1. Job postings by industry  
2. Salary distributions  
3. Remote vs. on-site job proportions

---

### 5.1 Job Postings by Industry

Understanding industry demand helps reveal which sectors are most active in hiring.

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Load dataset
df = pd.read_csv("data/lightcast_job_postings.csv", low_memory=False)

# Ensure /figures directory exists
os.makedirs("figures", exist_ok=True)

# col
industry_col = "NAICS_2022_6_NAME"

# Clean up column names
df.columns = df.columns.str.strip()

#drop unclassified
df = df[~df["NAICS_2022_6_NAME"].str.lower().str.contains("unclassified", na=False)]

# Count top 10 industries
top_industries = df[industry_col].value_counts().head(10)


plt.figure(figsize=(10, 6))
sns.barplot(x=top_industries.values, y=top_industries.index, orient="h")
plt.title("Top 10 Industries by Job Postings")
plt.xlabel("Number of Job Postings")
plt.ylabel("Industry")
plt.tight_layout()
plt.savefig("figures/industry_postings.png", dpi=300)
plt.show()
```

### 5.2 Salary Distribution by Industry
```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

os.makedirs("figures", exist_ok=True)

df = pd.read_csv("data/lightcast_job_postings.csv", low_memory=False)

# Define column names
industry_col = "NAICS_2022_6_NAME"
salary_col = "SALARY"

# Filter and clean salary
df = df[df[salary_col] > 0]
df = df[~df[industry_col].str.lower().str.contains("unclassified")]

# Compute average salary by industry
avg_salary = (
    df.groupby(industry_col)[salary_col]
    .mean()
    .sort_values(ascending=False)
    .head(10)
)

# Visual
plt.figure(figsize=(10, 6))
sns.barplot(
    x=avg_salary.values,
    y=avg_salary.index,
    palette="crest"
)
plt.title("Average Salary by Industry (Excluding Unclassified)")
plt.xlabel("Average Salary ($)")
plt.ylabel("Industry")
plt.tight_layout()

# save
plt.savefig("figures/average_salary_by_industry.png", dpi=300, bbox_inches="tight")
plt.show()
```
### 5.3 Remote vs. On-Site Jobs
```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

os.makedirs("figures", exist_ok=True)

df = pd.read_csv("data/lightcast_job_postings.csv", low_memory=False)

# Clean the REMOTE_TYPE_NAME column
df["REMOTE_TYPE_NAME"] = (
    df["REMOTE_TYPE_NAME"]
    .astype(str).str.strip().str.lower()
    .replace({"[none]": None, "none": None, "unknown": None, "nan": None, "na": None, "null": None, "": None})
)

# Count each type
remote_counts = df["REMOTE_TYPE_NAME"].value_counts()

# Visual
plt.figure(figsize=(6, 6))
plt.pie(
    remote_counts.values,
    labels=remote_counts.index,
    autopct="%1.1f%%",
    startangle=90,
    wedgeprops={"edgecolor": "white"}
)
plt.title("Remote vs. On-Site Job Distribution", fontsize=13)
plt.tight_layout()
plt.savefig("figures/remote_vs_onsite.png", dpi=300, bbox_inches="tight")
plt.show()
```