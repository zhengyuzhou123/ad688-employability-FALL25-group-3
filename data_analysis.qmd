
---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis"
author:
  - name: "Bingrui Qiao"
    affiliations: "Boston University"
bibliography: references.bib
csl: csl/econometrica.csl
format:
  html:
    toc: true
    number-sections: true
execute:
  echo: true
  eval: true
  freeze: false
  error: false
  cache: false
  enabled: !expr (os.getenv("CI", "false") != "true")
jupyter: python3
---



---

## 4. Data Cleaning & Preprocessing

In this section, we clean the Lightcast dataset, log each step, and save a reproducible cleaned CSV for downstream EDA.
```{python}
# --- 0.0 Ensure dependencies are installed (for CI / Quarto Cloud) ---
import sys, subprocess

# package
for pkg in ["gdown", "pandas", "matplotlib", "missingno", "pyarrow"]:
    subprocess.run([sys.executable, "-m", "pip", "install", pkg], check=False)
```
```{python}
import os, datetime
os.makedirs("logs", exist_ok=True)
with open("logs/_ping.txt", "w", encoding="utf-8") as f:
    f.write("hello @ " + str(datetime.datetime.now()) + "\n")
print("WROTE: logs/_ping.txt")
```

```{python}
# --- 4.0 Setup & Load (clean version) ---
import os, datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import missingno as msno
import subprocess

# Handle missingno (optional)
try:
    import missingno as msno
    HAS_MSNO = True
except ImportError:
    HAS_MSNO = False

# Paths
DATA_PATH  = "data/lightcast_job_postings.csv"
CLEAN_PATH = "data/cleaned_lightcast.csv"
LOG_PATH   = "logs/cleaning_log.txt"
FIG_MISS   = "figures/missing_values_heatmap.png"

# Ensure output dirs exist
os.makedirs("logs", exist_ok=True)
os.makedirs("figures", exist_ok=True)

# Logger
def log(msg: str):
    print(msg)
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(msg.rstrip() + "\n")

# Start a fresh log
with open(LOG_PATH, "w", encoding="utf-8") as f:
    f.write("=== DATA CLEANING LOG START ===\n")

# Ping file (to confirm write permission)
with open("logs/_ping.txt", "w", encoding="utf-8") as f:
    f.write("hello from python @ " + str(datetime.datetime.now()) + "\n")

# AUTO-DOWNLOAD IF MISSING
if not os.path.exists(DATA_PATH):
    log(f"⚠️ Dataset missing at {DATA_PATH}. Attempting to download from Google Drive...")
    gdrive_url = "https://drive.google.com/uc?id=1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ"
    try:
        import gdown
        gdown.download(gdrive_url, DATA_PATH, quiet=False)
        log("✅ Dataset downloaded successfully.")
    except Exception as e:
        raise FileNotFoundError(f"❌ Could not download dataset.\nError: {e}")


# Load data
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"❌ Dataset not found at {DATA_PATH}. Check path & working dir.")
df = pd.read_csv(DATA_PATH, low_memory=False, on_bad_lines="skip")
log(f"Loaded dataset → rows: {len(df):,}, cols: {df.shape[1]}")
```


```{python}
# --- 4.1 Drop redundant/irrelevant columns ---

columns_to_drop = [
"ID","URL","ACTIVE_URLS","DUPLICATES","LAST_UPDATED_TIMESTAMP",
"NAICS2","NAICS3","NAICS4","NAICS5","NAICS6",
"SOC_2","SOC_3","SOC_5"
]
before_cols = df.shape[1]
df.drop(columns=columns_to_drop, inplace=True, errors="ignore")
after_cols = df.shape[1]
log(f"Dropped {before_cols - after_cols} columns; remaining columns: {after_cols}")

# Normalize names & basic types

df.columns = [c.strip() for c in df.columns]

if "POSTED" in df.columns:
        df["POSTED"] = pd.to_datetime(df["POSTED"], errors="coerce")

if "Salary" in df.columns:
        df["Salary"] = pd.to_numeric(
            df["Salary"].astype(str).str.replace(r"[^0-9.-]", "", regex=True),
                errors="coerce"
)
```

```{python}
# --- 4.2 Visualize & handle missing values ---


if HAS_MSNO:
    plt.figure(figsize=(10,6))
    msno.heatmap(df)
    plt.title("Missing Values Heatmap", fontsize=14)
    plt.tight_layout()
    plt.savefig(FIG_MISS, dpi=150)
    plt.show()
    log(f"Saved missingness heatmap → {FIG_MISS}")
else:
    log("missingno not installed; skipping heatmap.")

# Log top-20 missing columns

na_pct = df.isna().mean().sort_values(ascending=False) * 100
log("Top-20 columns by missing%:")
for col, pct in na_pct.head(20).items():
    log(f"  - {col}: {pct:.1f}%")

# Drop columns with >50% missing

thresh = len(df) * 0.5
before_cols = df.shape[1]
df.dropna(axis=1, thresh=thresh, inplace=True)
after_cols = df.shape[1]
log(f"Dropped {before_cols - after_cols} high-missing columns (>50%). Remaining: {after_cols}")

# Impute numeric

if "Salary" in df.columns:
    med_salary = df["Salary"].median()
    df["Salary"].fillna(med_salary, inplace=True)
    log(f"Filled Salary NAs with median: {med_salary:.2f}")

# Impute categorical

for cat in ["Industry","REMOTE_TYPE_NAME","LOCATION"]:
    if cat in df.columns:
        df[cat] = df[cat].fillna("Unknown").astype(str).str.strip()
        log(f"Filled {cat} NAs with 'Unknown'")
```

```{python}
# --- 4.3 Remove duplicates ---


subset_cols = [c for c in ["TITLE","COMPANY","LOCATION","POSTED"] if c in df.columns]
before = len(df)
if subset_cols:
    df.drop_duplicates(subset=subset_cols, keep="first", inplace=True)
    after = len(df)
    log(f"Removed duplicates by {subset_cols}: {before - after} rows dropped; remaining: {after}")
```

```{python}
# --- 4.4 Optional salary sanity filter ---


if "Salary" in df.columns:
    bad = (df["Salary"] < 1) | (df["Salary"] > 1_000_000)
    n_bad = int(bad.sum())
    if n_bad > 0:
        df.loc[bad, "Salary"] = np.nan
        med2 = df["Salary"].median()
        df["Salary"].fillna(med2, inplace=True)
        log(f"Clipped extreme Salary ({n_bad} rows) and refilled with median {med2:.2f}")
```


```{python}
# --- 4.5 Save & summary ---


df.to_csv(CLEAN_PATH, index=False)
log(f"Saved cleaned dataset → {CLEAN_PATH}")

summary = f"Rows: {len(df):,}\nColumns: {df.shape[1]}\nSample columns: {list(df.columns)[:12]}"
print(summary)
log("✅ Cleaning pipeline finished successfully.")
```

